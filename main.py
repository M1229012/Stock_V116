# -*- coding: utf-8 -*-
"""
V116.21 Backend Core (Public Safe Version)
åŠŸèƒ½ï¼š
1. [è³‡æ–™ç”¢å‡º] è² è²¬æŠ“å–æ‰€æœ‰è‚¡å¸‚æ•¸æ“šï¼Œå¡«å…¥ STATS_HEADERS æŒ‡å®šçš„æ¬„ä½ã€‚
2. [é¡åº¦æ§ç®¡] è¨­å®š MAX_API_CALLS = 450ï¼Œè¶…éå³åˆ»å­˜æª”ä¸‹ç­ï¼Œç­‰å¾…ä¸‹å°æ™‚ Cron Job å–šé†’ã€‚
3. [åˆ†æ™‚ç­–ç•¥] 
   - 15:00~20:59ï¼šå…¨åŠ›æŠ“å–åŸºæœ¬ç›¤ (åƒ¹/é‡/PE/PB)ï¼Œç•¶æ²–ç‡æš«å¡« 0ã€‚
   - 21:00~23:59ï¼šæª¢æŸ¥ä»Šæ—¥å·²æ›´æ–°ä½†ç•¶æ²–ç‚º 0 è€…ï¼Œè£œæŠ“ç•¶æ²–æ•¸æ“šã€‚
4. [è³‡å®‰ä¿è­·] æ‰€æœ‰æ•æ„Ÿé‡‘é‘°çš†é€éç’°å¢ƒè®Šæ•¸è®€å–ï¼Œç¨‹å¼ç¢¼å…§ç„¡æ•æ„Ÿè³‡è¨Šã€‚
"""

import os
import sys
import time
import pandas as pd
import numpy as np
import requests
import re
import gspread
import logging
import urllib3
from google.oauth2.service_account import Credentials
from google.auth import default
from datetime import datetime, timedelta, time as dt_time, date
from dateutil.relativedelta import relativedelta
from zoneinfo import ZoneInfo

# è‡ªå‹•å®‰è£ç¼ºå°‘çš„å¥—ä»¶
try:
    import twstock
    import yfinance as yf
except ImportError:
    os.system('pip install twstock yfinance gspread google-auth python-dateutil requests pandas zoneinfo --quiet')
    import twstock
    import yfinance as yf

# ==========================================
# 1. è¨­å®šèˆ‡å¸¸æ•¸
# ==========================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
logger = logging.getLogger('yfinance')
logger.setLevel(logging.CRITICAL)
logger.disabled = True

UNIT_LOT = 1000
MAX_API_CALLS_PER_RUN = 450  # ğŸ”¥ é¡åº¦ä¸Šé™ï¼š450æ¬¡ (FinMind é™åˆ¶ç´„ 600/hr)

# å¾Œç«¯ç”¢å‡ºæ¬„ä½æ¨™æº–
STATS_HEADERS = [
    'ä»£è™Ÿ', 'åç¨±', 'é€£çºŒå¤©æ•¸', 'è¿‘30æ—¥æ³¨æ„æ¬¡æ•¸', 'è¿‘10æ—¥æ³¨æ„æ¬¡æ•¸', 'æœ€è¿‘ä¸€æ¬¡æ—¥æœŸ',
    '30æ—¥ç‹€æ…‹ç¢¼', '10æ—¥ç‹€æ…‹ç¢¼', 'æœ€å¿«è™•ç½®å¤©æ•¸', 'è™•ç½®è§¸ç™¼åŸå› ', 'é¢¨éšªç­‰ç´š', 'è§¸ç™¼æ¢ä»¶',
    'ç›®å‰åƒ¹', 'è­¦æˆ’åƒ¹', 'å·®å¹…(%)', 'ç›®å‰é‡', 'è­¦æˆ’é‡', 'æˆäº¤å€¼(å„„)',
    'é€±è½‰ç‡(%)', 'PE', 'PB', 'ç•¶æ²–ä½”æ¯”(%)'
]

# Sheet è¨­å®š (è‹¥éœ€æ›´é«˜éš±ç§ï¼Œå¯å°‡åç¨±æ”¹ç‚º os.getenv('SHEET_NAME'))
SHEET_NAME = "å°è‚¡æ³¨æ„è‚¡è³‡æ–™åº«_V33"
PARAM_SHEET_NAME = "å€‹è‚¡åƒæ•¸"

# æ™‚å€è¨­å®š
try: TW_TZ = ZoneInfo("Asia/Taipei")
except: TW_TZ = ZoneInfo("UTC")

TARGET_DATE = datetime.now(TW_TZ)
IS_AFTER_9PM = TARGET_DATE.hour >= 21  # åˆ¤æ–·æ˜¯å¦ç‚ºæ™šä¸Š9é»å¾Œ

SAFE_CRAWL_TIME = dt_time(19, 0)
SAFE_MARKET_OPEN_CHECK = dt_time(16, 30)

# ==========================================
# 2. API è¨­å®š (å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼Œå®‰å…¨)
# ==========================================
FINMIND_API_URL = "https://api.finmindtrade.com/api/v4/data"
# ğŸ”¥ é€™è£¡è®€å–ç’°å¢ƒè®Šæ•¸ï¼Œæ‰€ä»¥ç¨‹å¼ç¢¼å…¬é–‹ä¹Ÿæ²’é—œä¿‚
FINMIND_TOKEN = os.getenv('FinMind_1') or os.getenv('FinMind_2')

_FINMIND_CACHE = {}
API_CALL_COUNT = 0

# ============================
# 3. å·¥å…·å‡½å¼
# ============================
CN_NUM = {"ä¸€":"1","äºŒ":"2","ä¸‰":"3","å››":"4","äº”":"5","å…­":"6","ä¸ƒ":"7","å…«":"8","ä¹":"9","å":"10"}
KEYWORD_MAP = {"èµ·è¿„å…©å€‹ç‡Ÿæ¥­æ—¥": 11, "ç•¶æ—¥æ²–éŠ·": 13, "å€Ÿåˆ¸è³£å‡º": 12, "ç´¯ç©é€±è½‰ç‡": 10, "é€±è½‰ç‡": 4, "æˆäº¤é‡": 9, "æœ¬ç›Šæ¯”": 6, "è‚¡åƒ¹æ·¨å€¼æ¯”": 6, "æº¢æŠ˜åƒ¹": 8, "æ”¶ç›¤åƒ¹æ¼²è·Œç™¾åˆ†æ¯”": 1, "æœ€å¾Œæˆäº¤åƒ¹æ¼²è·Œ": 1, "æœ€è¿‘å…­å€‹ç‡Ÿæ¥­æ—¥ç´¯ç©": 1}

def normalize_clause_text(s: str) -> str:
    if not s: return ""
    s = str(s).replace("ç¬¬ã„§æ¬¾", "ç¬¬ä¸€æ¬¾")
    for cn, dg in CN_NUM.items(): s = s.replace(f"ç¬¬{cn}æ¬¾", f"ç¬¬{dg}æ¬¾")
    return s.translate(str.maketrans("ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼", "1234567890"))

def parse_clause_ids_strict(clause_text):
    if not isinstance(clause_text, str): return set()
    clause_text = normalize_clause_text(clause_text)
    ids = set(int(m) for m in re.findall(r'ç¬¬\s*(\d+)\s*æ¬¾', clause_text))
    if not ids:
        for k, v in KEYWORD_MAP.items():
            if k in clause_text: ids.add(v)
    return ids

def merge_clause_text(a, b):
    ids = parse_clause_ids_strict(a) | parse_clause_ids_strict(b)
    return "ã€".join([f"ç¬¬{x}æ¬¾" for x in sorted(ids)]) if ids else (a if len(a or "") >= len(b or "") else b)

def is_valid_accumulation_day(ids): return any(1 <= x <= 8 for x in ids)
def is_special_risk_day(ids): return any(9 <= x <= 14 for x in ids)
def get_ticker_suffix(market): return '.TWO' if any(k in str(market).upper() for k in ['ä¸Šæ«ƒ', 'TWO', 'TPEX', 'OTC']) else '.TW'

def get_or_create_ws(sh, title, headers=None, rows=5000, cols=20):
    try:
        ws = sh.worksheet(title)
        if headers and ws.col_count < len(headers): ws.resize(rows=ws.row_count, cols=len(headers))
        return ws
    except:
        ws = sh.add_worksheet(title=title, rows=str(rows), cols=str(cols))
        if headers: ws.append_row(headers, value_input_option="USER_ENTERED")
        return ws

# ============================
# 4. API æ ¸å¿ƒ (è¨ˆæ•¸å™¨ + å–®ä¸€ Token + å»¶é²)
# ============================
def finmind_get(dataset, data_id=None, start_date=None, end_date=None):
    global API_CALL_COUNT
    
    cache_key = (dataset, data_id, start_date, end_date)
    if cache_key in _FINMIND_CACHE: return _FINMIND_CACHE[cache_key].copy()

    # é¡åº¦ä¿è­·
    if API_CALL_COUNT >= MAX_API_CALLS_PER_RUN:
        return pd.DataFrame()

    params = {"dataset": dataset}
    if data_id: params["data_id"] = str(data_id)
    if start_date: params["start_date"] = start_date
    if end_date: params["end_date"] = end_date
    
    headers = {"User-Agent": "Mozilla/5.0"}
    if FINMIND_TOKEN: headers["Authorization"] = f"Bearer {FINMIND_TOKEN}"

    for _ in range(3): # Retry
        API_CALL_COUNT += 1
        try:
            time.sleep(1.5) # å¼·åˆ¶å»¶é²
            r = requests.get(FINMIND_API_URL, params=params, headers=headers, timeout=10, verify=False)
            if r.status_code == 200:
                j = r.json()
                df = pd.DataFrame(j.get("data", []))
                if len(_FINMIND_CACHE) >= 2000: _FINMIND_CACHE.clear()
                _FINMIND_CACHE[cache_key] = df
                return df.copy()
            elif r.status_code == 429:
                print("âš ï¸ FinMind Rate Limit Reached.")
                API_CALL_COUNT = MAX_API_CALLS_PER_RUN + 1
                return pd.DataFrame()
            else:
                time.sleep(2)
        except: time.sleep(1)
    return pd.DataFrame()

# ============================
# 5. è³‡æ–™è™•ç†é‚è¼¯
# ============================
def parse_roc_date(s):
    try: p=s.strip().split('/'); return date(int(p[0])+1911, int(p[1]), int(p[2]))
    except: return None

def parse_jail_period(s):
    if not s: return None, None
    d = s.split('ï½') if 'ï½' in s else s.split('~')
    if len(d)<2 and '-' in s: d = s.split('-')
    if len(d)>=2:
        s_d, e_d = parse_roc_date(d[0]), parse_roc_date(d[1])
        if s_d and e_d: return s_d, e_d
    return None, None

def get_jail_map(sd, ed):
    print("ğŸ”’ ä¸‹è¼‰è™•ç½®åå–®...")
    jm = {}
    try:
        url = "https://www.twse.com.tw/rwd/zh/announcement/punish"
        r = requests.get(url, params={"startDate":sd.strftime("%Y%m%d"),"endDate":ed.strftime("%Y%m%d"),"response":"json"}, verify=False)
        for row in r.json().get("tables", [{}])[0].get("data", []):
            try:
                s, e = parse_jail_period(row[6])
                if s and e: jm.setdefault(row[2].strip(), []).append((s, e))
            except: continue
    except: pass
    return jm

def get_last_n_non_jail_trade_dates(code, cal, jm, ex, n=30):
    last_end = date(1900,1,1)
    if jm and code in jm: last_end = jm[code][-1][1]
    picked = []
    for d in reversed(cal):
        if d <= last_end: break
        if ex.get(code) and d in ex[code]: continue
        if jm and code in jm:
            is_j = False
            for s,e in jm[code]: 
                if s<=d<=e: is_j=True; break
            if is_j: continue
        picked.append(d)
        if len(picked)>=n: break
    return list(reversed(picked))

def fetch_history_data(code):
    try:
        time.sleep(1) 
        df = yf.Ticker(code).history(period="1y", auto_adjust=False)
        if not df.empty and df.index.tz: df.index = df.index.tz_localize(None)
        return df
    except: return pd.DataFrame()

def fetch_stock_fundamental(ticker_code):
    data = {'pe': 0, 'pb': 0}
    try:
        t = yf.Ticker(ticker_code)
        data['pe'] = t.info.get('trailingPE', t.info.get('forwardPE', 0)) or 0
        data['pb'] = t.info.get('priceToBook', 0) or 0
        data['pe'] = round(data['pe'], 2)
        data['pb'] = round(data['pb'], 2)
    except: pass
    return data

def get_daytrade_stats_finmind(stock_id, date_str):
    # åˆ†æµæ ¸å¿ƒï¼š9é»å‰ä¸æŠ“ç•¶æ²–ï¼Œçœé¡åº¦
    if not IS_AFTER_9PM: 
        return 0.0, 0.0
    
    start = (datetime.strptime(date_str, "%Y-%m-%d") - timedelta(days=15)).strftime("%Y-%m-%d")
    d = finmind_get("TaiwanStockDayTrading", data_id=stock_id, start_date=start, end_date=date_str)
    p = finmind_get("TaiwanStockPrice", data_id=stock_id, start_date=start, end_date=date_str)

    if p.empty or d.empty: return 0.0, 0.0
    try:
        m = pd.merge(p[['date','Trading_Volume']], d[['date','Volume']], on='date')
        if m.empty: return 0.0, 0.0
        m['date'] = pd.to_datetime(m['date']); m=m.sort_values('date')
        r6 = m.tail(6)
        if len(r6)<1: return 0.0, 0.0
        last = r6.iloc[-1]
        
        td = (last['Volume']/last['Trading_Volume']*100) if last['Trading_Volume']>0 else 0.0
        
        sum_vol = r6['Volume'].sum()
        sum_total = r6['Trading_Volume'].sum()
        avg = (sum_vol/sum_total*100) if sum_total>0 else 0.0
        
        return round(td, 2), round(avg, 2)
    except: return 0.0, 0.0

# ============================
# 6. é¢¨éšªè¨ˆç®— (å¡«æ»¿æ‰€æœ‰æ¬„ä½)
# ============================
def calculate_full_risk(stock_id, hist_df, fund_data, est_days, dt_today, dt_avg6, shares=1):
    res = {
        'risk_level': 'ä½', 'trigger_msg': '', 'curr_price': 0, 
        'limit_price': 0, 'gap_pct': 999.0, 'curr_vol': 0, 'limit_vol': 0, 
        'turnover_val': 0, 'turnover_rate': 0, 
        'pe': fund_data.get('pe', 0), 'pb': fund_data.get('pb', 0), 
        'day_trade_pct': dt_today
    }

    if hist_df.empty: return res

    curr_close = float(hist_df.iloc[-1]['Close'])
    curr_vol_shares = float(hist_df.iloc[-1]['Volume'])
    
    res['curr_price'] = round(curr_close, 2)
    res['curr_vol'] = int(curr_vol_shares / 1000)
    res['turnover_val'] = round((curr_close * curr_vol_shares) / 100000000, 2)
    
    if shares > 1:
        res['turnover_rate'] = round((curr_vol_shares / shares) * 100, 2)
    
    if est_days <= 1: res['risk_level'] = 'é«˜'
    elif est_days <= 2: res['risk_level'] = 'ä¸­'

    if len(hist_df) >= 7:
        ref_price = float(hist_df.iloc[-7]['Close'])
        res['limit_price'] = round(ref_price * 1.32, 2)
        if curr_close > 0:
            res['gap_pct'] = round(((res['limit_price'] - curr_close) / curr_close) * 100, 1)
            
    if len(hist_df) >= 60:
        avg_vol = hist_df['Volume'].iloc[-60:].mean()
        res['limit_vol'] = int((avg_vol * 5) / 1000)

    return res

# ============================
# 7. ä¸»ç¨‹å¼
# ============================
def connect_google_sheets():
    try:
        key = "/service_key.json" if os.path.exists("/service_key.json") else "service_key.json"
        if not os.path.exists(key): return None, None
        gc = gspread.service_account(filename=key)
        try: sh = gc.open(SHEET_NAME)
        except: sh = gc.create(SHEET_NAME)
        return sh, None
    except: return None, None

def main():
    print(f"ğŸš€ å•Ÿå‹• V116.21 | æ™‚é–“: {TARGET_DATE} | 9PMæ¨¡å¼: {IS_AFTER_9PM}")
    
    sh, _ = connect_google_sheets()
    if not sh: return

    # 1. è®€å–ç¾æœ‰è³‡æ–™
    ws_stats = get_or_create_ws(sh, "è¿‘30æ—¥ç†±é–€çµ±è¨ˆ", headers=STATS_HEADERS)
    existing_data = ws_stats.get_all_records()
    
    today_str = TARGET_DATE.strftime("%Y-%m-%d")
    
    # 2. å»ºç«‹æ›´æ–°æª¢æŸ¥è¡¨ (åˆ†æµé‚è¼¯)
    target_stocks_info = []
    
    for row in existing_data:
        code = str(row.get('ä»£è™Ÿ'))
        if not code: continue
        
        last_date = str(row.get('æœ€è¿‘ä¸€æ¬¡æ—¥æœŸ'))
        dt_pct = row.get('ç•¶æ²–ä½”æ¯”(%)')
        try: dt_val = float(dt_pct)
        except: dt_val = 0.0
        
        mode = "SKIP"
        if last_date != today_str:
            mode = "FULL" # éœ€è¦æ›´æ–°è‚¡åƒ¹
        elif IS_AFTER_9PM and dt_val == 0:
            mode = "DT_ONLY" # è‚¡åƒ¹å·²æ›´æ–°ï¼Œåªè£œç•¶æ²–
            
        if mode != "SKIP":
            target_stocks_info.append({'code': code, 'mode': mode, 'data': row})

    print(f"ğŸ“‹ å¾…è™•ç†: {len(target_stocks_info)} æª” | é¡åº¦: {MAX_API_CALLS_PER_RUN}")

    # 3. è¼‰å…¥åŸºæœ¬åƒæ•¸
    precise_db = {}
    try:
        ws_param = sh.worksheet(PARAM_SHEET_NAME)
        for r in ws_param.get_all_records():
            precise_db[str(r.get('ä»£è™Ÿ'))] = {"market": r.get('å¸‚å ´','ä¸Šå¸‚'), "shares": r.get('ç™¼è¡Œè‚¡æ•¸',1)}
    except: pass
    
    # 4. é–‹å§‹åŸ·è¡Œ
    updates = []
    processed = 0
    
    for item in target_stocks_info:
        if API_CALL_COUNT >= MAX_API_CALLS_PER_RUN:
            print("ğŸ›‘ é¡åº¦ç”¨ç›¡ï¼Œåœæ­¢åŸ·è¡Œï¼Œç­‰å¾…ä¸‹æ¬¡æ’ç¨‹ã€‚")
            break
            
        code = item['code']
        mode = item['mode']
        old_data = item['data']
        
        print(f"   [{processed+1}] {code} ({mode})...")
        
        suffix = get_ticker_suffix(precise_db.get(code, {}).get('market', 'ä¸Šå¸‚'))
        ticker = f"{code}{suffix}"
        
        # è‚¡åƒ¹èˆ‡åŸºæœ¬é¢ (Yahoo)
        hist = fetch_history_data(ticker)
        fund = fetch_stock_fundamental(ticker)

        # ç•¶æ²– (FinMind)
        dt_today, dt_avg6 = get_daytrade_stats_finmind(code, today_str)
        
        # è¨ˆç®—
        shares = 1
        try: shares = int(str(precise_db.get(code, {}).get('shares', 1)).replace(',',''))
        except: pass
        
        est_days = 99
        try: est_days = int(old_data.get('æœ€å¿«è™•ç½®å¤©æ•¸', 99))
        except: pass
        
        risk_res = calculate_full_risk(code, hist, fund, est_days, dt_today, dt_avg6, shares)
        
        # æ›´æ–°æ¬„ä½
        new_row = old_data.copy()
        new_row['æœ€è¿‘ä¸€æ¬¡æ—¥æœŸ'] = today_str
        for k, v in risk_res.items():
            # å°æ‡‰ STATS_HEADERS çš„æ¬„ä½åç¨±åš mapping
            # (risk_res key) -> (Sheet Header)
            map_key = {
                'curr_price': 'ç›®å‰åƒ¹', 'limit_price': 'è­¦æˆ’åƒ¹', 'gap_pct': 'å·®å¹…(%)',
                'curr_vol': 'ç›®å‰é‡', 'limit_vol': 'è­¦æˆ’é‡', 'turnover_val': 'æˆäº¤å€¼(å„„)',
                'turnover_rate': 'é€±è½‰ç‡(%)', 'pe': 'PE', 'pb': 'PB', 'day_trade_pct': 'ç•¶æ²–ä½”æ¯”(%)'
            }
            if k in map_key:
                new_row[map_key[k]] = v
        
        updates.append(new_row)
        processed += 1
        
    # 5. å¯«å›
    if updates:
        print("ğŸ’¾ å„²å­˜è³‡æ–™ä¸­...")
        update_map = {row['ä»£è™Ÿ']: row for row in updates}
        final_rows = []
        for row in existing_data:
            code = str(row.get('ä»£è™Ÿ'))
            # å¦‚æœæœ‰æ›´æ–°å°±ç”¨æ–°çš„ï¼Œæ²’æœ‰å°±ç”¨èˆŠçš„
            target = update_map.get(code, row)
            # è½‰æˆ list æº–å‚™å¯«å…¥
            final_rows.append([target.get(h, '') for h in STATS_HEADERS])
                
        ws_stats.clear()
        ws_stats.append_row(STATS_HEADERS, value_input_option='USER_ENTERED')
        ws_stats.append_rows(final_rows, value_input_option='USER_ENTERED')
        
    print(f"\nâœ… åŸ·è¡ŒçµæŸã€‚æ›´æ–°: {processed} ç­†ã€‚APIä½¿ç”¨: {API_CALL_COUNT}")

if __name__ == "__main__":
    main()
