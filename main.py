# -*- coding: utf-8 -*-
"""
V116.18 å°è‚¡æ³¨æ„è‚¡ç³»çµ± (GitHub Action å–®æª”ç›´ä¸Šç‰ˆ - å›è£œå¯é åº¦å¼·åŒ–) + è¿‘90æ—¥è™•ç½®è‚¡å°ˆå€æ•´åˆç‰ˆ
ä¿®æ­£é‡é»ï¼š
1. [å¿«å–] jail_map æ”¹ç”± Google Sheetã€Œè™•ç½®è‚¡90æ—¥æ˜ç´°ã€è®€å– (é©æ‡‰ä¸­æ–‡æ¬„ä½)ã€‚
2. [å„ªåŒ–] Playwright æ””æˆªæ¢ä»¶æ”¾å¯¬ï¼Œç§»é™¤ json å­—ä¸²æª¢æŸ¥ã€‚
3. [é™¤éŒ¯] ç§»é™¤å¤šé¤˜çš„ return èˆ‡å¢åŠ  stock_calendar ç©ºå€¼ä¿è­·ã€‚
4. [ä¿®æ­£] TPEx æ¬„ä½è‡ªå‹•åµæ¸¬ (Index 0 or 1)ï¼Œè§£æ±ºè³‡æ–™éŒ¯ä½è¢«æ¿¾æ‰çš„å•é¡Œã€‚
5. [ä¿®æ­£] ä¸Šå¸‚ä»£è™Ÿè£œå…¨é‚è¼¯å¼·åŒ–ï¼Œç¢ºä¿è³‡æ–™å®Œæ•´æ€§ã€‚
"""

import os
import twstock
import yfinance as yf
import pandas as pd
import numpy as np
import requests
import re
import time
import random
import gspread
import logging
import asyncio
import nest_asyncio
from google.oauth2.service_account import Credentials
from datetime import datetime, timedelta, time as dt_time, date
from dateutil.relativedelta import relativedelta
from zoneinfo import ZoneInfo
from playwright.async_api import async_playwright

nest_asyncio.apply()

# ==========================================
# 1. è¨­å®šéœéŸ³æ¨¡å¼èˆ‡å¸¸æ•¸
# ==========================================
logger = logging.getLogger('yfinance')
logger.setLevel(logging.CRITICAL)
logger.disabled = True

UNIT_LOT = 1000

# å®šç¾©çµ±è¨ˆè¡¨é ­
STATS_HEADERS = [
    'ä»£è™Ÿ', 'åç¨±', 'é€£çºŒå¤©æ•¸', 'è¿‘30æ—¥æ³¨æ„æ¬¡æ•¸', 'è¿‘10æ—¥æ³¨æ„æ¬¡æ•¸', 'æœ€è¿‘ä¸€æ¬¡æ—¥æœŸ',
    '30æ—¥ç‹€æ…‹ç¢¼', '10æ—¥ç‹€æ…‹ç¢¼', 'æœ€å¿«è™•ç½®å¤©æ•¸', 'è™•ç½®è§¸ç™¼åŸå› ', 'é¢¨éšªç­‰ç´š', 'è§¸ç™¼æ¢ä»¶',
    'ç›®å‰åƒ¹', 'è­¦æˆ’åƒ¹', 'å·®å¹…(%)', 'ç›®å‰é‡', 'è­¦æˆ’é‡', 'æˆäº¤å€¼(å„„)',
    'é€±è½‰ç‡(%)', 'PE', 'PB', 'ç•¶æ²–ä½”æ¯”(%)'
]

# ==========================================
# ğŸ“† è¨­å®šå€
# ==========================================
SHEET_NAME = "å°è‚¡æ³¨æ„è‚¡è³‡æ–™åº«_V33"
PARAM_SHEET_NAME = "å€‹è‚¡åƒæ•¸"
TW_TZ = ZoneInfo("Asia/Taipei")
TARGET_DATE = datetime.now(TW_TZ)

# æ™‚é–“é–€æª»
SAFE_CRAWL_TIME = dt_time(17, 30)        
DAYTRADE_PUBLISH_TIME = dt_time(21, 0)   
SAFE_MARKET_OPEN_CHECK = dt_time(16, 30) 

IS_NIGHT_RUN = TARGET_DATE.hour >= 20
IS_AFTER_SAFE = TARGET_DATE.time() >= SAFE_CRAWL_TIME
IS_AFTER_DAYTRADE = TARGET_DATE.time() >= DAYTRADE_PUBLISH_TIME

# å›è£œåƒæ•¸
MAX_BACKFILL_TRADING_DAYS = 40   
VERIFY_RECENT_DAYS = 2           

# ==========================================
# ğŸ”‘ FinMind é‡‘é‘°è¨­å®š
# ==========================================
FINMIND_API_URL = "https://api.finmindtrade.com/api/v4/data"

token1 = os.getenv('FinMind_1')
token2 = os.getenv('FinMind_2')
FINMIND_TOKENS = [t for t in [token1, token2] if t]

CURRENT_TOKEN_INDEX = 0
_FINMIND_CACHE = {}

print(f"ğŸš€ å•Ÿå‹• V116.18 å°è‚¡æ³¨æ„è‚¡ç³»çµ± (Fix: TPEx Auto-Detect & TWSE Patch)")
print(f"ğŸ•’ ç³»çµ±æ™‚é–“ (Taiwan): {TARGET_DATE.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"â° æ™‚åºç‹€æ…‹: After 17:30? {IS_AFTER_SAFE} | After 21:00? {IS_AFTER_DAYTRADE}")

try: twstock.__update_codes()
except: pass

# ============================
# ğŸ› ï¸ å·¥å…·å‡½å¼
# ============================
CN_NUM = {"ä¸€":"1","äºŒ":"2","ä¸‰":"3","å››":"4","äº”":"5","å…­":"6","ä¸ƒ":"7","å…«":"8","ä¹":"9","å":"10"}

KEYWORD_MAP = {
    "èµ·è¿„å…©å€‹ç‡Ÿæ¥­æ—¥": 11, "ç•¶æ—¥æ²–éŠ·": 13, "å€Ÿåˆ¸è³£å‡º": 12, "ç´¯ç©é€±è½‰ç‡": 10, "é€±è½‰ç‡": 4,
    "æˆäº¤é‡": 9, "æœ¬ç›Šæ¯”": 6, "è‚¡åƒ¹æ·¨å€¼æ¯”": 6, "æº¢æŠ˜åƒ¹": 8, "æ”¶ç›¤åƒ¹æ¼²è·Œç™¾åˆ†æ¯”": 1,
    "æœ€å¾Œæˆäº¤åƒ¹æ¼²è·Œ": 1, "æœ€è¿‘å…­å€‹ç‡Ÿæ¥­æ—¥ç´¯ç©": 1
}

def normalize_clause_text(s: str) -> str:
    if not s: return ""
    s = str(s)
    s = s.replace("ç¬¬ã„§æ¬¾", "ç¬¬ä¸€æ¬¾")
    for cn, dg in CN_NUM.items():
        s = s.replace(f"ç¬¬{cn}æ¬¾", f"ç¬¬{dg}æ¬¾")
    s = s.translate(str.maketrans("ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼", "1234567890"))
    return s

def parse_clause_ids_strict(clause_text):
    if not isinstance(clause_text, str): return set()
    clause_text = normalize_clause_text(clause_text)
    ids = set()
    matches = re.findall(r'ç¬¬\s*(\d+)\s*æ¬¾', clause_text)
    for m in matches: ids.add(int(m))
    if not ids:
        for keyword, code in KEYWORD_MAP.items():
            if keyword in clause_text: ids.add(code)
    return ids

def merge_clause_text(a, b):
    ids = set()
    ids |= parse_clause_ids_strict(a) if a else set()
    ids |= parse_clause_ids_strict(b) if b else set()
    if ids: return "ã€".join([f"ç¬¬{x}æ¬¾" for x in sorted(ids)])
    a = a or ""; b = b or ""
    return a if len(a) >= len(b) else b

def is_valid_accumulation_day(ids):
    if not ids: return False
    return any(1 <= x <= 8 for x in ids)

def is_special_risk_day(ids):
    if not ids: return False
    return any(9 <= x <= 14 for x in ids)

def get_ticker_suffix(market_type):
    m = str(market_type).upper().strip()
    keywords = ['ä¸Šæ«ƒ', 'TWO', 'TPEX', 'OTC']
    if any(k in m for k in keywords): return '.TWO'
    return '.TW'

def connect_google_sheets():
    try:
        if not os.path.exists("service_key.json"): return None, None
        gc = gspread.service_account(filename="service_key.json")
        try: sh = gc.open(SHEET_NAME)
        except: sh = gc.create(SHEET_NAME)
        return sh, None
    except: return None, None

def get_or_create_ws(sh, title, headers=None, rows=5000, cols=20):
    need_cols = max(cols, len(headers) if headers else 0)
    try:
        ws = sh.worksheet(title)
        try:
            if headers and ws.col_count < need_cols:
                ws.resize(rows=ws.row_count, cols=need_cols)
        except: pass
        return ws
    except:
        print(f"âš ï¸ å·¥ä½œè¡¨ '{title}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨å»ºç«‹...")
        ws = sh.add_worksheet(title=title, rows=str(rows), cols=str(need_cols))
        if headers:
            ws.append_row(headers, value_input_option="USER_ENTERED")
        return ws

def load_log_index(ws_log):
    existing_keys = set()
    date_counts = {}
    try:
        vals = ws_log.get_all_values()
        if not vals or len(vals) <= 1: return existing_keys, date_counts
        for r in vals[1:]:
            if len(r) >= 3 and str(r[0]).strip():
                d = str(r[0]).strip()
                code = str(r[2]).strip().replace("'", "")
                if code:
                    k = d + "_" + code
                    existing_keys.add(k)
                    date_counts[d] = date_counts.get(d, 0) + 1
    except: pass
    return existing_keys, date_counts

def load_status_index(ws_status):
    key_to_row = {}
    cnt_map = {}
    try:
        vals = ws_status.get_all_values()
        if not vals or len(vals) <= 1: return key_to_row, cnt_map
        for r_idx, row in enumerate(vals[1:], start=2):
            if len(row) >= 1 and str(row[0]).strip():
                d = str(row[0]).strip()
                key_to_row[d] = r_idx
                c = 0
                if len(row) >= 2:
                    try: c = int(str(row[1]).strip())
                    except: c = 0
                cnt_map[d] = c
    except: pass
    return key_to_row, cnt_map

def upsert_status(ws_status, key_to_row, date_str, count, now_str):
    row_data = [date_str, int(count), now_str]
    if date_str in key_to_row:
        r = key_to_row[date_str]
        try: ws_status.update(values=[row_data], range_name=f"A{r}:C{r}", value_input_option="USER_ENTERED")
        except: pass
    else:
        try: ws_status.append_row(row_data, value_input_option="USER_ENTERED")
        except: pass

def finmind_get(dataset, data_id=None, start_date=None, end_date=None):
    global CURRENT_TOKEN_INDEX
    cache_key = (dataset, data_id, start_date, end_date)
    if cache_key in _FINMIND_CACHE: return _FINMIND_CACHE[cache_key].copy()

    params = {"dataset": dataset}
    if data_id: params["data_id"] = str(data_id)
    if start_date: params["start_date"] = start_date
    if end_date: params["end_date"] = end_date
    if not FINMIND_TOKENS: return pd.DataFrame()

    for _ in range(4):
        headers = {"Authorization": f"Bearer {FINMIND_TOKENS[CURRENT_TOKEN_INDEX]}", "User-Agent": "Mozilla/5.0", "Connection": "close"}
        try:
            r = requests.get(FINMIND_API_URL, params=params, headers=headers, timeout=10)
            if r.status_code == 200:
                j = r.json()
                df = pd.DataFrame(j.get("data", [])) if "data" in j else pd.DataFrame()
                if len(_FINMIND_CACHE) >= 2000: _FINMIND_CACHE.clear()
                _FINMIND_CACHE[cache_key] = df
                return df.copy()
            elif r.status_code != 200:
                time.sleep(2)
                CURRENT_TOKEN_INDEX = (CURRENT_TOKEN_INDEX + 1) % len(FINMIND_TOKENS)
                continue
        except: time.sleep(1)
    return pd.DataFrame()

def update_market_monitoring_log(sh):
    print("ğŸ“Š æª¢æŸ¥ä¸¦æ›´æ–°ã€Œå¤§ç›¤æ•¸æ“šç›£æ§ã€...")
    HEADERS = ['æ—¥æœŸ', 'ä»£è™Ÿ', 'åç¨±', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œå¹…(%)', 'æˆäº¤é‡‘é¡(å„„)']
    ws_market = get_or_create_ws(sh, "å¤§ç›¤æ•¸æ“šç›£æ§", headers=HEADERS, cols=10)

    def norm_date(s):
        s = str(s).strip()
        if not s: return ""
        try: return pd.to_datetime(s, errors='coerce').strftime("%Y-%m-%d")
        except: return s

    key_to_row = {}
    try:
        all_vals = ws_market.get_all_values()
        for r_idx, row in enumerate(all_vals[1:], start=2):
            if len(row) >= 2:
                key_to_row[f"{norm_date(row[0])}_{str(row[1]).strip()}"] = r_idx
    except: pass

    existing_keys = set(key_to_row.keys())

    try:
        targets = [
            {'fin_id': 'TAIEX', 'code': '^TWII', 'name': 'åŠ æ¬ŠæŒ‡æ•¸'},
            {'fin_id': 'TPEx',  'code': '^TWOII', 'name': 'æ«ƒè²·æŒ‡æ•¸'}
        ]
        start_date_str = (TARGET_DATE - timedelta(days=45)).strftime("%Y-%m-%d")
        dfs = {}
        for t in targets:
            df = finmind_get("TaiwanStockPrice", data_id=t['fin_id'], start_date=start_date_str)
            if not df.empty:
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
                df.index = df.index.tz_localize(None)
                if 'close' in df.columns:
                    df['Close'] = df['close'].astype(float)
                    df['Pct'] = df['Close'].pct_change() * 100
                if 'Turnover' in df.columns: df['Volume'] = df['Turnover'].astype(float)
                elif 'Trading_money' in df.columns: df['Volume'] = df['Trading_money'].astype(float)
                else: df['Volume'] = 0.0
                dfs[t['code']] = df

        new_rows = []
        today_str = TARGET_DATE.strftime("%Y-%m-%d")
        all_dates = set()
        for df in dfs.values(): all_dates.update(df.index.strftime("%Y-%m-%d").tolist())

        for d in sorted(all_dates):
            for t in targets:
                code = t['code']; name = t['name']
                df = dfs.get(code)
                if df is None or d not in df.index.strftime("%Y-%m-%d"): continue
                try: row = df.loc[d]
                except: row = df[df.index.strftime("%Y-%m-%d") == d].iloc[0]

                if pd.isna(row.get('Close')): continue
                close = round(float(row['Close']), 2)
                pct = round(float(row.get('Pct', 0) or 0), 2)
                vol = round(float(row.get('Volume', 0) or 0) / 100000000, 2)

                row_data = [d, code, name, close, pct, vol]
                comp_key = f"{d}_{code}"

                if d == today_str and TARGET_DATE.time() < SAFE_MARKET_OPEN_CHECK: continue
                if d == today_str and comp_key in key_to_row and TARGET_DATE.time() >= SAFE_MARKET_OPEN_CHECK:
                    try:
                        r_num = key_to_row[comp_key]
                        ws_market.update(values=[row_data], range_name=f'A{r_num}:F{r_num}', value_input_option="USER_ENTERED")
                    except: pass
                    continue
                if comp_key in existing_keys: continue
                if close > 0: new_rows.append(row_data)

        if new_rows: ws_market.append_rows(new_rows, value_input_option="USER_ENTERED")
    except Exception as e: print(f" âŒ å¤§ç›¤æ›´æ–°å¤±æ•—: {e}")

# ============================
# ğŸ”¥ è™•ç½®è³‡æ–™ç›¸é—œå‡½å¼ (Jail)
# ============================
def parse_roc_date(roc_date_str):
    try:
        roc_date_str = str(roc_date_str).strip()
        parts = re.split(r"[/-]", roc_date_str)
        if len(parts) == 3:
            y = int(parts[0]) + 1911
            m = int(parts[1])
            d = int(parts[2])
            return date(y, m, d)
    except:
        return None
    return None

def parse_jail_period(period_str):
    if not period_str:
        return None, None

    s = str(period_str).strip()
    dates = []
    if "ï½" in s:
        dates = s.split("ï½")
    elif "~" in s:
        dates = s.split("~")
    elif "-" in s and "/" in s and s.count("-") == 1:
        dates = s.split("-")

    if len(dates) >= 2:
        sd = parse_roc_date(dates[0].strip())
        ed = parse_roc_date(dates[1].strip())
        if sd and ed:
            return sd, ed
    return None, None

# âœ… [ä¿®æ­£] å¾ Google Sheet è®€å–è™•ç½®è‚¡å¿«å– (ä½¿ç”¨ä¸­æ–‡æ¬„ä½)
def get_jail_map_from_sheet(sh):
    print("ğŸ“‚ å¾ Google Sheet è®€å–è™•ç½®åå–®å¿«å– (è™•ç½®è‚¡90æ—¥æ˜ç´°)...")
    jail_map = {}
    try:
        ws = sh.worksheet("è™•ç½®è‚¡90æ—¥æ˜ç´°")
        rows = ws.get_all_records()
        for r in rows:
            # âœ… æ”¹ç”¨ä¸­æ–‡ Key
            code = str(r.get('ä»£è™Ÿ', '')).strip()
            if not code: 
                # å…¼å®¹èˆŠç‰ˆè‹±æ–‡ Key
                code = str(r.get('Code', '')).strip()
            
            if not code: continue
            
            period = str(r.get('è™•ç½®æœŸé–“', '')).strip()
            if not period:
                period = str(r.get('Period', '')).strip()

            sd, ed = parse_jail_period(period)
            if sd and ed:
                jail_map.setdefault(code, []).append((sd, ed))
        print(f"âœ… å¿«å–è®€å–å®Œæˆï¼Œå…± {len(jail_map)} æª”è™•ç½®è‚¡è³‡æ–™ã€‚")
    except Exception as e:
        print(f"âš ï¸ è®€å–è™•ç½®å¿«å–å¤±æ•— (å¯èƒ½æ˜¯åˆæ¬¡åŸ·è¡Œæˆ–å·¥ä½œè¡¨ä¸å­˜åœ¨): {e}")
    return jail_map

def is_in_jail(stock_id, target_date, jail_map):
    if not jail_map or stock_id not in jail_map:
        return False
    for s, e in jail_map[stock_id]:
        if s <= target_date <= e:
            return True
    return False

def prev_trade_date(d, cal_dates):
    try:
        idx = cal_dates.index(d)
        return cal_dates[idx - 1] if idx > 0 else None
    except:
        for i in range(len(cal_dates) - 1, -1, -1):
            if cal_dates[i] < d:
                return cal_dates[i]
        return None

def build_exclude_map(cal_dates, jail_map):
    exclude_map = {}
    if not jail_map:
        return exclude_map

    for code, periods in jail_map.items():
        s = set()
        for start, end in periods:
            # 2) è™•ç½®å‰ä¸€æ—¥
            pd = prev_trade_date(start, cal_dates)
            if pd:
                s.add(pd)
            # 1) è™•ç½®æœŸé–“ï¼ˆåªæ”¾äº¤æ˜“æ—¥ï¼‰
            for d in cal_dates:
                if start <= d <= end:
                    s.add(d)
        exclude_map[code] = s
    return exclude_map

def is_excluded(code, d, exclude_map):
    return bool(exclude_map) and (code in exclude_map) and (d in exclude_map[code])

def get_last_n_non_jail_trade_dates(stock_id, cal_dates, jail_map, exclude_map=None, n=30):
    # ğŸ”¥ å‰›å‡ºé—œæ­¸é›¶ï¼šåªçœ‹æœ€è¿‘ä¸€æ¬¡è™•ç½®çµæŸæ—¥
    last_jail_end = date(1900, 1, 1)
    if jail_map and stock_id in jail_map and jail_map[stock_id]:
        last_jail_end = jail_map[stock_id][-1][1]

    picked = []
    for d in reversed(cal_dates):
        # âœ… å‰›å‡ºé—œå‰å…¨éƒ¨ä¸è¦
        if d <= last_jail_end:
            break
        if is_excluded(stock_id, d, exclude_map):
            continue
        if jail_map and is_in_jail(stock_id, d, jail_map):
            continue
        picked.append(d)
        if len(picked) >= n:
            break
            
    window = cal_dates[-n:] if len(cal_dates) >= n else cal_dates
    picked = [d for d in window if d > last_jail_end]

    return list(reversed(picked))

# ============================
# ğŸ”¥ æ¯æ—¥å…¬å‘Šçˆ¬èŸ²å€ (TWSE / TPEx)
# ============================
def fetch_twse_attention_rows(date_obj, date_str):
    date_str_nodash = date_obj.strftime("%Y%m%d")
    rows = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(
            "https://www.twse.com.tw/rwd/zh/announcement/notice",
            params={"startDate": date_str_nodash, "endDate": date_str_nodash, "response": "json"},
            headers=headers,
            timeout=10,
        )
        if r.status_code != 200:
            return None # è¦–ç‚ºå¤±æ•—

        d = r.json()
        for i in d.get("data", []) or []:
            code = str(i[1]).strip()
            name = str(i[2]).strip()
            if len(code) == 4 and code.isdigit():
                raw = " ".join([str(x) for x in i])
                ids = parse_clause_ids_strict(raw)
                c_str = "ã€".join([f"ç¬¬{k}æ¬¾" for k in sorted(ids)]) or raw
                rows.append({"æ—¥æœŸ": date_str, "å¸‚å ´": "TWSE", "ä»£è™Ÿ": code, "åç¨±": name, "è§¸çŠ¯æ¢æ¬¾": c_str})
    except:
        return None # ç•°å¸¸è¦–ç‚ºå¤±æ•—
    return rows

def fetch_tpex_attention_rows(date_obj, date_str):
    roc_date = f"{date_obj.year - 1911}/{date_obj.month:02d}/{date_obj.day:02d}"
    url = "https://www.tpex.org.tw/www/zh-tw/bulletin/attention"

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://www.tpex.org.tw/",
        "Origin": "https://www.tpex.org.tw",
        "Accept": "application/json, text/plain, */*",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "X-Requested-With": "XMLHttpRequest",
    }
    payload = {"date": roc_date, "response": "json"}

    s = requests.Session()

    try:
        s.get("https://www.tpex.org.tw/", headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
    except:
        pass

    for attempt in range(1, 4):
        try:
            r = s.post(url, data=payload, headers=headers, timeout=12)
            if r.status_code != 200:
                time.sleep(0.8)
                continue

            res = r.json()

            target = []
            if "tables" in res:
                for t in res["tables"]:
                    target.extend(t.get("data", []) or [])
            else:
                target = res.get("data", []) or []

            rows = []
            for i in target:
                if len(i) <= 5:
                    continue
                row_date = str(i[5]).strip()
                if row_date not in (roc_date, date_str):
                    continue

                code = str(i[1]).strip()
                name = str(i[2]).strip()
                if not (code.isdigit() and len(code) == 4):
                    continue

                raw = " ".join([str(x) for x in i])
                ids = parse_clause_ids_strict(raw)
                c_str = "ã€".join([f"ç¬¬{k}æ¬¾" for k in sorted(ids)]) if ids else raw

                rows.append({"æ—¥æœŸ": date_str, "å¸‚å ´": "TPEx", "ä»£è™Ÿ": code, "åç¨±": name, "è§¸çŠ¯æ¢æ¬¾": c_str})

            return rows
        except:
            time.sleep(0.8)

    return None

def get_daily_data(date_obj):
    date_str = date_obj.strftime("%Y-%m-%d")
    print(f"ğŸ“¡ çˆ¬å–å…¬å‘Š {date_str}...")

    twse_rows = fetch_twse_attention_rows(date_obj, date_str)
    tpex_rows = fetch_tpex_attention_rows(date_obj, date_str)

    if twse_rows is None or tpex_rows is None:
        print("âŒ æŠ“å–å¤±æ•—ï¼ˆå›å‚³ Noneï¼‰ï¼Œæœ¬è¼ªä¸å¯«å…¥ç‹€æ…‹ï¼Œç•™å¾…ä¸‹æ¬¡å›è£œ")
        return None

    rows = []
    rows.extend(twse_rows)
    rows.extend(tpex_rows)

    if rows:
        print(f"âœ… æŠ“åˆ° {len(rows)} æª”")
    else:
        print("âš ï¸ ç„¡è³‡æ–™")
    return rows

def backfill_daily_logs(sh, ws_log, cal_dates, target_trade_date_obj):
    now_str = TARGET_DATE.strftime("%Y-%m-%d %H:%M:%S")
    existing_keys, date_counts = load_log_index(ws_log)
    ws_status = get_or_create_ws(sh, "çˆ¬å–ç‹€æ…‹", headers=["æ—¥æœŸ", "æŠ“åˆ°æª”æ•¸", "æœ€å¾Œæ›´æ–°æ™‚é–“"], cols=5)
    key_to_row, status_cnt = load_status_index(ws_status)
    status_is_new = (len(status_cnt) == 0)

    # if not status_is_new: ...

    key_to_row, status_cnt = load_status_index(ws_status)
    window_dates = cal_dates[-MAX_BACKFILL_TRADING_DAYS:] if len(cal_dates) > MAX_BACKFILL_TRADING_DAYS else cal_dates[:]
    recent_dates = cal_dates[-VERIFY_RECENT_DAYS:] if len(cal_dates) >= VERIFY_RECENT_DAYS else cal_dates[:]
    dates_to_check = sorted(set(window_dates + recent_dates))

    rows_to_append = []
    status_updates = []

    print(f"ğŸ§© å›è£œæª¢æŸ¥ï¼šå…± {len(dates_to_check)} å€‹äº¤æ˜“æ—¥ï¼ˆå«æœ€è¿‘ {VERIFY_RECENT_DAYS} æ—¥å¼·åˆ¶é©—è­‰ï¼‰")

    for d in dates_to_check:
        d_str = d.strftime("%Y-%m-%d")

        if d == TARGET_DATE.date() and TARGET_DATE.time() < SAFE_CRAWL_TIME: continue

        log_cnt = int(date_counts.get(d_str, 0))
        st_cnt = status_cnt.get(d_str, None)
        need_fetch = False

        if d in recent_dates: need_fetch = True
        if (st_cnt is not None) and (log_cnt < int(st_cnt)): need_fetch = True
        if (st_cnt is None) and (log_cnt == 0): need_fetch = True
        if (st_cnt is None) and (d in window_dates): need_fetch = True

        if not need_fetch: continue

        data = get_daily_data(d)

        # âœ… è‹¥å›å‚³ None ä»£è¡¨æŠ“å–å¤±æ•—ï¼Œè·³éä¸æ›´æ–°ç‹€æ…‹
        if data is None:
            print(f"âš ï¸ {d_str} æŠ“å–å¤±æ•—(None)ï¼Œè·³éä¸æ›´æ–°ç‹€æ…‹")
            continue

        official_cnt = len(data)

        for s in data:
            k = f"{s['æ—¥æœŸ']}_{s['ä»£è™Ÿ']}"
            if k not in existing_keys:
                rows_to_append.append([s['æ—¥æœŸ'], s['å¸‚å ´'], f"'{s['ä»£è™Ÿ']}", s['åç¨±'], s['è§¸çŠ¯æ¢æ¬¾']])
                existing_keys.add(k)
                date_counts[s['æ—¥æœŸ']] = date_counts.get(s['æ—¥æœŸ'], 0) + 1

        status_updates.append((d_str, official_cnt, st_cnt))

    if rows_to_append:
        print(f"ğŸ’¾ å›è£œå¯«å…¥ã€Œæ¯æ—¥ç´€éŒ„ã€ï¼š{len(rows_to_append)} ç­†")
        ws_log.append_rows(rows_to_append, value_input_option="USER_ENTERED")
    else:
        print("âœ… æ¯æ—¥ç´€éŒ„ç„¡éœ€å›è£œå¯«å…¥")

    key_to_row, status_cnt = load_status_index(ws_status)
    for d_str, official_cnt, old_st_cnt in status_updates:
        write_cnt = official_cnt
        if official_cnt == 0:
            if old_st_cnt is not None and int(old_st_cnt) > 0: write_cnt = int(old_st_cnt)
            elif int(date_counts.get(d_str, 0)) > 0: write_cnt = int(date_counts[d_str])
        upsert_status(ws_status, key_to_row, d_str, write_cnt, now_str)

def is_market_open_by_finmind(date_str):
    df = finmind_get("TaiwanStockPrice", data_id="2330", start_date=date_str, end_date=date_str)
    return not df.empty

def get_official_trading_calendar(days=60):
    end = TARGET_DATE.strftime("%Y-%m-%d")
    start = (TARGET_DATE - timedelta(days=days*2)).strftime("%Y-%m-%d")
    print("ğŸ“… ä¸‹è¼‰æ—¥æ›†...")
    df = finmind_get("TaiwanStockTradingDate", start_date=start, end_date=end)
    dates = []
    if not df.empty:
        df['date'] = pd.to_datetime(df['date']).dt.date
        dates = sorted(df['date'].tolist())
    else:
        curr = TARGET_DATE.date()
        while len(dates) < days:
            if curr.weekday() < 5: dates.append(curr)
            curr -= timedelta(days=1)
        dates = sorted(dates)

    today_date = TARGET_DATE.date()
    today_str = today_date.strftime("%Y-%m-%d")
    is_late_enough = TARGET_DATE.time() > SAFE_MARKET_OPEN_CHECK

    if dates and today_date > dates[-1] and today_date.weekday() < 5:
        if is_late_enough:
            print(f"âš ï¸ æ—¥æ›†ç¼ºæ¼ä»Šæ—¥ ({today_date})ï¼Œé©—è­‰é–‹å¸‚ä¸­...")
            if is_market_open_by_finmind(today_str):
                print(f"âœ… é©—è­‰æˆåŠŸ (2330æœ‰åƒ¹)ï¼Œè£œå…¥ä»Šæ—¥ã€‚")
                dates.append(today_date)
            else:
                print(f"â›” é©—è­‰å¤±æ•— (2330ç„¡åƒ¹)ï¼Œåˆ¤æ–·ç‚ºä¼‘å¸‚æˆ–è³‡æ–™æœªæ›´æ–°ï¼Œä¸è£œå…¥ã€‚")
        else:
            print(f"â³ æ™‚é–“å°šæ—©ï¼Œæš«ä¸å¼·åˆ¶è£œå…¥ä»Šæ—¥æ—¥æ›†ã€‚")

    return dates[-days:]

def get_daytrade_stats_finmind(stock_id, target_date_str):
    end = target_date_str
    start = (datetime.strptime(target_date_str, "%Y-%m-%d") - timedelta(days=15)).strftime("%Y-%m-%d")
    df_dt = finmind_get("TaiwanStockDayTrading", stock_id, start_date=start, end_date=end)
    df_p = finmind_get("TaiwanStockPrice", stock_id, start_date=start, end_date=end)

    if df_dt.empty or df_p.empty: return None, None
    try:
        m = pd.merge(df_p[['date', 'Trading_Volume']], df_dt[['date', 'Volume']], on='date', how='inner')
        if m.empty: return None, None
        m = m.sort_values('date')
        last = m.iloc[-1]
        td = (last['Volume']/last['Trading_Volume']*100) if last['Trading_Volume']>0 else 0
        avg = m.tail(6); sum_v = avg['Volume'].sum(); sum_t = avg['Trading_Volume'].sum()
        avg_td = (sum_v/sum_t*100) if sum_t>0 else 0
        return round(td, 2), round(avg_td, 2)
    except: return None, None

def fetch_history_data(ticker_code):
    try:
        df = yf.Ticker(ticker_code).history(period="1y", auto_adjust=False)
        if df.empty: return pd.DataFrame()
        df.index = df.index.tz_localize(None)
        return df
    except: return pd.DataFrame()

def load_precise_db_from_sheet(sh):
    try:
        ws = sh.worksheet(PARAM_SHEET_NAME)
        data = ws.get_all_records()
        db = {}
        for row in data:
            code = str(row.get('ä»£è™Ÿ', '')).strip()
            if not code: continue
            try: shares = int(str(row.get('ç™¼è¡Œè‚¡æ•¸', 1)).replace(',', ''))
            except: shares = 1
            try: offset = float(row.get('é¡è‚¡æ¼²å¹…ä¿®æ­£', 0.0))
            except: offset = 0.0
            try: turn_avg = float(row.get('åŒé¡è‚¡å¹³å‡é€±è½‰', 5.0))
            except: turn_avg = 5.0
            try: purity = float(row.get('æˆäº¤é‡ç´”åº¦', 1.0))
            except: purity = 1.0
            market = str(row.get('å¸‚å ´', 'ä¸Šå¸‚')).strip()
            db[code] = {"market": market, "shares": shares, "sector_offset": offset, "sector_turn_avg": turn_avg, "vol_purity": purity}
        return db
    except: return {}

def fetch_stock_fundamental(stock_id, ticker_code, precise_db):
    market = 'ä¸Šå¸‚'; shares = 0
    if str(stock_id) in precise_db:
        db = precise_db[str(stock_id)]
        market = db['market']; shares = db['shares']
    data = {'shares': shares, 'market_type': market, 'pe': -1, 'pb': -1}
    try:
        t = yf.Ticker(ticker_code)
        if ".TWO" in ticker_code: data['market_type'] = 'ä¸Šæ«ƒ'
        if data['shares'] <= 1:
            s = t.fast_info.get('shares', None)
            if s: data['shares'] = int(s)
        data['pe'] = t.info.get('trailingPE', t.info.get('forwardPE', 0))
        data['pb'] = t.info.get('priceToBook', 0)
        if data['pe']: data['pe'] = round(data['pe'], 2)
        if data['pb']: data['pb'] = round(data['pb'], 2)
    except: pass
    return data

def calc_pct(curr, ref):
    return ((curr - ref) / ref) * 100 if ref != 0 else 0

def calculate_full_risk(stock_id, hist_df, fund_data, est_days, dt_today_pct, dt_avg6_pct):
    res = {'risk_level': 'ä½', 'trigger_msg': '', 'curr_price': 0, 'limit_price': 0, 'gap_pct': 999.0, 'curr_vol': 0, 'limit_vol': 0, 'turnover_val': 0, 'turnover_rate': 0, 'pe': fund_data.get('pe', 0), 'pb': fund_data.get('pb', 0), 'day_trade_pct': dt_today_pct, 'is_triggered': False}
    if hist_df.empty or len(hist_df) < 7:
        if est_days <= 1: res['risk_level'] = 'é«˜'
        elif est_days <= 2: res['risk_level'] = 'ä¸­'
        return res

    curr_close = float(hist_df.iloc[-1]['Close'])
    curr_vol_shares = float(hist_df.iloc[-1]['Volume'])
    curr_vol_lots = int(curr_vol_shares / UNIT_LOT)
    shares = fund_data.get('shares', 1)
    if shares > 1: turnover = (curr_vol_shares / shares) * 100
    else: turnover = -1.0
    turnover_val_money = curr_close * curr_vol_shares

    res['curr_price'] = round(curr_close, 2)
    res['curr_vol'] = curr_vol_lots
    res['turnover_rate'] = round(turnover, 2)
    res['turnover_val'] = round(turnover_val_money / 100000000, 2)

    triggers = []
    if curr_close < 5: return res

    window_7 = hist_df.tail(7)
    ref_6 = float(window_7.iloc[0]['Close'])
    rise_6 = calc_pct(curr_close, ref_6)
    price_diff_6 = abs(curr_close - ref_6)

    cond_1 = rise_6 > 32
    cond_2 = (rise_6 > 25) and (price_diff_6 >= 50)
    if cond_1: triggers.append(f"ã€ç¬¬ä¸€æ¬¾ã€‘6æ—¥æ¼²{rise_6:.1f}%(>32%)")
    elif cond_2: triggers.append(f"ã€ç¬¬ä¸€æ¬¾ã€‘6æ—¥æ¼²{rise_6:.1f}%ä¸”åƒ¹å·®{price_diff_6:.0f}å…ƒ")

    limit_p = ref_6 * 1.32
    if cond_2: limit_p = min(limit_p, ref_6 * 1.25)
    res['limit_price'] = round(limit_p, 2)
    res['gap_pct'] = round(((limit_p - curr_close)/curr_close)*100, 1)

    if len(hist_df)>=31 and calc_pct(curr_close, float(hist_df.iloc[-31]['Close'])) > 100: triggers.append("ã€ç¬¬äºŒæ¬¾ã€‘30æ—¥æ¼²>100%")
    if len(hist_df)>=61 and calc_pct(curr_close, float(hist_df.iloc[-61]['Close'])) > 130: triggers.append("ã€ç¬¬äºŒæ¬¾ã€‘60æ—¥æ¼²>130%")
    if len(hist_df)>=91 and calc_pct(curr_close, float(hist_df.iloc[-91]['Close'])) > 160: triggers.append("ã€ç¬¬äºŒæ¬¾ã€‘90æ—¥æ¼²>160%")

    if len(hist_df) >= 61:
        avg_vol_60 = hist_df['Volume'].iloc[-61:-1].mean()
        if avg_vol_60 > 0:
            vol_ratio = curr_vol_shares / avg_vol_60
            res['limit_vol'] = int(avg_vol_60 * 5 / 1000)
            if turnover >= 0.1 and curr_vol_lots >= 500:
                if rise_6 > 25 and vol_ratio > 5: triggers.append(f"ã€ç¬¬ä¸‰æ¬¾ã€‘æ¼²{rise_6:.0f}%+é‡{vol_ratio:.1f}å€")

    if turnover > 10 and rise_6 > 25: triggers.append(f"ã€ç¬¬å››æ¬¾ã€‘æ¼²{rise_6:.0f}%+è½‰{turnover:.0f}%")

    if len(hist_df) >= 61:
        avg_vol_60 = hist_df['Volume'].iloc[-61:-1].mean()
        avg_vol_6 = hist_df['Volume'].iloc[-6:].mean()
        is_exclude = (turnover < 0.1) or (curr_vol_lots < 500) or (turnover_val_money < 30000000)
        if not is_exclude and avg_vol_60 > 0:
            r1 = avg_vol_6 / avg_vol_60
            r2 = curr_vol_shares / avg_vol_60
            if r1 > 5: triggers.append(f"ã€ç¬¬ä¹æ¬¾ã€‘6æ—¥å‡é‡æ”¾å¤§{r1:.1f}å€")
            if r2 > 5: triggers.append(f"ã€ç¬¬ä¹æ¬¾ã€‘ç•¶æ—¥é‡æ”¾å¤§{r2:.1f}å€")

    if turnover > 0 and turnover_val_money >= 500000000:
        acc_turn = (hist_df['Volume'].iloc[-6:].sum() / shares) * 100
        if acc_turn > 50 and turnover > 10: triggers.append(f"ã€ç¬¬åæ¬¾ã€‘ç´¯è½‰{acc_turn:.0f}%")

    if len(hist_df) >= 6:
        gap = hist_df.iloc[-6:]['High'].max() - hist_df.iloc[-6:]['Low'].min()
        threshold = 100 + (int((curr_close - 500)/500)+1)*25 if curr_close >= 500 else 100
        if gap >= threshold: triggers.append(f"ã€ç¬¬åä¸€æ¬¾ã€‘6æ—¥åƒ¹å·®{gap:.0f}å…ƒ(>é–€æª»{threshold})")

    pending_msg = ""
    # âœ… ç•¶æ²–ç‡åˆ¤æ–·ï¼šæœªå…¬å¸ƒå‰‡åªæ¨™è¨˜ pendingï¼Œä¸ç®—è§¸ç™¼
    if dt_today_pct is None or dt_avg6_pct is None:
        pending_msg = "(ç•¶æ²–ç‡å¾…å…¬å¸ƒ)"
    else:
        dt_vol_est = curr_vol_shares * (dt_today_pct / 100.0)
        dt_vol_lots = dt_vol_est / 1000
        is_exclude = (turnover < 5) or (turnover_val_money < 500000000) or (dt_vol_lots < 5000)
        if not is_exclude:
            if dt_avg6_pct > 60 and dt_today_pct > 60:
                triggers.append(f"ã€ç¬¬åä¸‰æ¬¾ã€‘ç•¶æ²–{dt_today_pct}%(6æ—¥{dt_avg6_pct}%)")

    # âœ… æœ€å¾Œè¼¸å‡ºè¨Šæ¯ï¼šæœ‰è§¸ç™¼å°±åŠ ä¸Š pending è¨»è¨˜ï¼ˆè‹¥æœ‰ï¼‰
    if triggers:
        res['is_triggered'] = True
        res['risk_level'] = 'é«˜'
        res['trigger_msg'] = "ä¸”".join(triggers) + (f" {pending_msg}" if pending_msg else "")
    else:
        # æ²’è§¸ç™¼å°±ä¸è¦å›  pending å‡ç´šé¢¨éšª
        res['trigger_msg'] = pending_msg
        if est_days <= 1: res['risk_level'] = 'é«˜'
        elif est_days <= 2: res['risk_level'] = 'ä¸­'
        elif est_days >= 3: res['risk_level'] = 'ä½'

    return res

def check_jail_trigger_now(status_list, clause_list):
    status_list = list(status_list); clause_list = list(clause_list)
    if len(status_list) < 30:
        pad = 30 - len(status_list)
        status_list = [0]*pad + status_list
        clause_list = [""]*pad + clause_list

    c1_streak = 0
    for c in clause_list[-3:]:
        if 1 in parse_clause_ids_strict(c): c1_streak += 1

    v5 = 0; v10 = 0; v30 = 0
    total = len(status_list)
    for i in range(30):
        idx = total - 1 - i
        if idx < 0: break
        if status_list[idx] == 1:
            ids = parse_clause_ids_strict(clause_list[idx])
            if is_valid_accumulation_day(ids):
                if i < 5: v5 += 1
                if i < 10: v10 += 1
                v30 += 1

    reasons = []
    if c1_streak == 3: reasons.append("å·²è§¸ç™¼(é€£3ç¬¬ä¸€æ¬¾)")
    if v5 == 5: reasons.append("å·²è§¸ç™¼(é€£5)")
    if v10 >= 6: reasons.append(f"å·²è§¸ç™¼(10æ—¥{v10}æ¬¡)")
    if v30 >= 12: reasons.append(f"å·²è§¸ç™¼(30æ—¥{v30}æ¬¡)")
    return (len(reasons) > 0), " | ".join(reasons)

def simulate_days_to_jail_strict(status_list, clause_list, *, stock_id=None, target_date=None, jail_map=None, enable_safe_filter=True):
    if stock_id and target_date and jail_map and is_in_jail(stock_id, target_date, jail_map):
        return 0, "è™•ç½®ä¸­"

    trigger_now, reason_now = check_jail_trigger_now(status_list, clause_list)
    if trigger_now:
        return 0, reason_now.replace("å·²è§¸ç™¼", "å·²é”æ¨™ï¼Œæ¬¡ä¸€ç‡Ÿæ¥­æ—¥è™•ç½®")

    if enable_safe_filter:
        recent_valid_10 = 0
        check_len = min(len(status_list), 10)
        if check_len > 0:
            for b, c in zip(status_list[-check_len:], clause_list[-check_len:]):
                if b == 1 and is_valid_accumulation_day(parse_clause_ids_strict(c)):
                    recent_valid_10 += 1
        if recent_valid_10 == 0: return 99, "X"

    status_list = list(status_list); clause_list = list(clause_list)
    if len(status_list) < 30:
        pad = 30 - len(status_list)
        status_list = [0]*pad + status_list
        clause_list = [""]*pad + clause_list

    days = 0
    while days < 10:
        days += 1
        status_list.append(1); clause_list.append("ç¬¬1æ¬¾")

        c1_streak = 0
        for c in clause_list[-3:]:
            if 1 in parse_clause_ids_strict(c): c1_streak += 1

        v5 = 0; v10 = 0; v30 = 0
        total = len(status_list)
        for i in range(30):
            idx = total - 1 - i
            if idx < 0: break
            if status_list[idx] == 1:
                ids = parse_clause_ids_strict(clause_list[idx])
                if is_valid_accumulation_day(ids):
                    if i < 5: v5 += 1
                    if i < 10: v10 += 1
                    v30 += 1

        reasons = []
        if c1_streak == 3: reasons.append(f"å†{days}å¤©è™•ç½®")
        if v5 == 5: reasons.append(f"å†{days}å¤©è™•ç½®(é€£5)")
        if v10 >= 6: reasons.append(f"å†{days}å¤©è™•ç½®(10æ—¥{v10}æ¬¡)")
        if v30 >= 12: reasons.append(f"å†{days}å¤©è™•ç½®(30æ—¥{v30}æ¬¡)")

        if reasons:
            return days, " | ".join(reasons)

    return 99, ""

# ==========================================
# ğŸ”¥ è™•ç½®è‚¡ 90 æ—¥æ˜ç´°çˆ¬èŸ²é‚è¼¯ (Playwright + API)
# ==========================================

# 1. ä¸Šæ«ƒ (TPEx) - API ç›´æ”» + åç¨±æ¸…ç†
def fetch_tpex_jail_90d(s_date, e_date):
    sd = s_date.strftime("%Y/%m/%d")
    ed = e_date.strftime("%Y/%m/%d")
    print(f"  [ä¸Šæ«ƒ] è«‹æ±‚: {sd} ~ {ed} ... ", end="")
    
    url = "https://www.tpex.org.tw/www/zh-tw/bulletin/disposal"
    payload = {"startDate": sd, "endDate": ed, "response": "json", "length": "5000", "start": "0"}
    headers = {"User-Agent": "Mozilla/5.0", "X-Requested-With": "XMLHttpRequest"}
    
    try:
        r = requests.post(url, data=payload, headers=headers, timeout=15)
        if r.status_code == 200:
            data = r.json()
            rows = data.get("tables", [{}])[0].get("data", []) or data.get("data", [])
            
            if rows:
                df = pd.DataFrame(rows)
                
                # âœ… [é—œéµä¿®æ­£] TPEx æ¬„ä½è‡ªå‹•åµæ¸¬
                # æœ‰äº›æ™‚å€™ API å›å‚³ [Seq, Code, Name...] (6 cols)ï¼Œæœ‰äº›æ™‚å€™ [Code, Name...] (5 cols)
                # æˆ‘å€‘æª¢æŸ¥ç¬¬ 0 æ¬„æ˜¯å¦ç‚º 4 ç¢¼æ•¸å­—ï¼Œè‹¥æ˜¯å‰‡ç‚º Codeï¼Œå¦å‰‡æª¢æŸ¥ç¬¬ 1 æ¬„
                
                code_idx = 1 # é è¨­å‡è¨­ Index 1 æ˜¯ä»£è™Ÿ (å¸¸è¦‹: [Seq, Code, Name...])
                
                # æ¸¬è©¦ç¬¬ä¸€ç­†è³‡æ–™çš„ç¬¬ 0 æ¬„æ˜¯å¦ç‚º 4 ä½æ•¸å­—
                first_row = df.iloc[0]
                if str(first_row[0]).strip().isdigit() and len(str(first_row[0]).strip()) == 4:
                    code_idx = 0 # ä¿®æ­£ç‚º Index 0
                
                # æ ¹æ“šåµæ¸¬çµæœé¸å–æ¬„ä½
                # Code, Name, Period, Reason
                target_indices = [code_idx, code_idx+1, code_idx+2, code_idx+3]
                
                if df.shape[1] > target_indices[-1]:
                    df = df.iloc[:, target_indices]
                    df.columns = ["Code", "Name", "Period", "Reason"]
                    df["Market"] = "ä¸Šæ«ƒ"
                    
                    # æ¸…ç†åç¨±ï¼šç§»é™¤æ‹¬è™Ÿèˆ‡ç¶²å€
                    df["Name"] = df["Name"].astype(str).apply(lambda x: x.split("(")[0].strip())
                    
                    # å¼·åˆ¶è½‰ç‚ºå­—ä¸²ä¸¦å»é™¤ç©ºç™½
                    df["Code"] = df["Code"].astype(str).str.strip()
                    
                    print(f"âœ… æˆåŠŸ ({len(df)} ç­†, Codeåœ¨ç¬¬{code_idx}æ¬„)")
                    return df
            print("âš ï¸ ç„¡è³‡æ–™")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
    return pd.DataFrame()

# 2. ä¸Šå¸‚ (TWSE) - æ¬„ä½ç²¾æº–å®šä½ (Index 6)
async def fetch_twse_playwright_90d(s_date, e_date):
    print(f"  [ä¸Šå¸‚] å•Ÿå‹•ç€è¦½å™¨æ¨¡æ“¬ (ä¿®æ­£ç´¢å¼• [2,3,6,7])...")
    sd_str = s_date.strftime("%Y%m%d")
    ed_str = e_date.strftime("%Y%m%d")
    
    url = "https://www.twse.com.tw/zh/announcement/punish.html"
    captured_data = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, args=['--no-sandbox'])
        page = await browser.new_page()
        
        async def handle_response(response):
            if "announcement/punish" in response.url:
                try:
                    data = await response.json()
                    if "data" in data:
                        captured_data.extend(data["data"])
                        print(f"    â””â”€â”€ âš¡ æ””æˆªè³‡æ–™: {len(data['data'])} ç­†")
                except: pass

        page.on("response", handle_response)

        try:
            await page.goto(url, wait_until="networkidle")
            await page.evaluate(f"""
                () => {{
                    document.querySelector('input[name="startDate"]').value = "{sd_str}";
                    document.querySelector('input[name="endDate"]').value = "{ed_str}";
                }}
            """)
            await page.locator("button.search").click()
            await page.wait_for_timeout(5000)
        except Exception as e:
            print(f"    âŒ æ“ä½œå¤±æ•—: {e}")
        
        await browser.close()

    if captured_data:
        df = pd.DataFrame(captured_data)
        
        # Index 2: Code (ä»£è™Ÿ)
        # Index 3: Name (åç¨±)
        # Index 6: Period (è™•ç½®æœŸé–“)  <-- td[7]
        # Index 7: Reason (è™•ç½®æªæ–½)
        
        if df.shape[1] >= 8:
            df = df.iloc[:, [2, 3, 6, 7]]
            df.columns = ["Code", "Name", "Period", "Reason"]
            df["Market"] = "ä¸Šå¸‚"
            print(f"    âœ… æ•´ç†å®Œæˆ ({len(df)} ç­†)")
            return df
        else:
            print(f"    âš ï¸ æ¬„ä½æ•¸é‡ä¸è¶³ ({df.shape[1]})ï¼Œç„¡æ³•å°æ‡‰")
            
    print("    âš ï¸ ç„¡è³‡æ–™")
    return pd.DataFrame()

async def run_jail_crawler_pipeline():
    """ æ•´åˆä¸Šå¸‚æ«ƒè¿‘ 90 æ—¥è™•ç½®è‚¡çˆ¬èŸ²æµç¨‹ """
    end_date = date.today()
    start_date = end_date - timedelta(days=150) # å¯¬é¬†ä¸€é»æŠ“ 150 å¤©ï¼Œç¯©é¸å¾Œå– 90
    print(f"ğŸ¯ å•Ÿå‹•å…¨å¸‚å ´è™•ç½®è‚¡æŠ“å–: {start_date} ~ {end_date}")

    all_dfs = []

    # 1. æŠ“ä¸Šæ«ƒ (åˆ†æ‰¹)
    curr = start_date
    while curr < end_date:
        next_date = min(curr + timedelta(days=45), end_date)
        df_tpex = fetch_tpex_jail_90d(curr, next_date)
        if not df_tpex.empty: all_dfs.append(df_tpex)
        curr = next_date + timedelta(days=1)
        time.sleep(0.5)

    # 2. æŠ“ä¸Šå¸‚ (ä¸€æ¬¡)
    df_twse = await fetch_twse_playwright_90d(start_date, end_date)
    if not df_twse.empty: all_dfs.append(df_twse)

    if all_dfs:
        print("\nğŸ”„ åˆä½µè™•ç½®è‚¡è³‡æ–™ä¸­...")
        final_df = pd.concat(all_dfs, ignore_index=True)
        
        # è½‰å­—ä¸²ä¸¦å»ç©ºç™½
        final_df["Code"] = final_df["Code"].astype(str).str.strip()
        final_df["Name"] = final_df["Name"].astype(str).str.strip()
        final_df["Period"] = final_df["Period"].astype(str).str.strip()

        # âœ… [æ–°å¢] ä¸Šå¸‚è³‡æ–™æ¸…æ´—ï¼šè‹¥ä»£è™Ÿç©ºç™½ï¼Œå˜—è©¦å¾åç¨±æå– (å¦‚ "1519 è¯åŸ")
        # ä½¿ç”¨ regex æå–é–‹é ­çš„æ•¸å­— (ç›¸å®¹å…¨å½¢/åŠå½¢ç©ºç™½)
        mask_empty_code = (final_df["Code"] == "")
        if mask_empty_code.any():
            print(f"âš ï¸ ç™¼ç¾ {mask_empty_code.sum()} ç­†ä»£è™Ÿç©ºç™½è³‡æ–™ï¼Œå˜—è©¦ä¿®å¾©...")
            # æå–åç¨±æ¬„ä½ä¸­çš„æ•¸å­—éƒ¨åˆ† (ä¾‹å¦‚ "1519" from "1519 è¯åŸ")
            extracted = final_df.loc[mask_empty_code, "Name"].str.extract(r'^(\d{4})')
            
            # è‹¥æå–æˆåŠŸï¼Œå¡«å…¥ Code
            final_df.loc[mask_empty_code, "Code"] = extracted[0].fillna("")
            
            # ä¿®æ­£ Name (ç§»é™¤å‰é¢çš„ä»£è™Ÿèˆ‡ç©ºç™½)
            final_df.loc[mask_empty_code, "Name"] = final_df.loc[mask_empty_code, "Name"].str.replace(r'^\d{4}\s+', '', regex=True)

        # âœ… [é—œéµå„ªåŒ–] åœ¨ regex ç¯©é¸å‰å†æ¬¡å¼·åˆ¶æ¸…é™¤ç©ºç™½
        final_df["Code"] = final_df["Code"].astype(str).str.strip()

        # âœ… ä¿®æ­£éœ€æ±‚ 1: åš´æ ¼ç¯©é¸åªæœ‰ 4 ä½æ•¸å­—çš„è‚¡ç¥¨ä»£è™Ÿ
        # éæ¿¾æ‰æ¬Šè­‰(6ç¢¼)ã€å¯è½‰å‚µ(5ç¢¼)æˆ–å…¶ä»–éå€‹è‚¡
        final_df = final_df[final_df["Code"].str.match(r'^\d{4}$')]
        
        # âœ… ä¿®æ­£éœ€æ±‚ 2: å»ºç«‹æ­£ç¢ºçš„æ’åºæ—¥æœŸ (è§£ææ°‘åœ‹å¹´ 114/xx/xx -> 2025xx)
        def parse_sort_date(period_str):
            try:
                # å–å€é–“çš„èµ·å§‹æ—¥ (ä¾‹å¦‚ "114/08/19~..." å– "114/08/19")
                start_part = period_str.replace("~", "-").split("-")[0].strip()
                # è™•ç†æ°‘åœ‹å¹´æ ¼å¼
                if "/" in start_part:
                    parts = start_part.split("/")
                    if len(parts) == 3:
                        y = int(parts[0]) + 1911
                        m = int(parts[1])
                        d = int(parts[2])
                        return f"{y}{m:02d}{d:02d}" # å›å‚³ YYYYMMDD å­—ä¸²
                return "99999999" # è§£æå¤±æ•—æ”¾æœ€å¾Œ
            except:
                return "99999999"

        final_df["SortDate"] = final_df["Period"].apply(parse_sort_date)
        
        # âœ… ä¿®æ­£éœ€æ±‚ 3: æ’åº (Oldest -> Newest)
        # ascending=[True, True] ä»£è¡¨æ—¥æœŸç”±å°(èˆŠ)åˆ°å¤§(æ–°)ï¼Œæ¯æ—¥æ›´æ–°å°±æœƒåœ¨æœ€ä¸‹é¢
        final_df.sort_values(by=["SortDate", "Code"], ascending=[True, True], inplace=True)
        final_df.drop_duplicates(subset=["Code", "Period", "Reason"], inplace=True)

        # âœ… ä¿®æ­£éœ€æ±‚ 4: åˆªé™¤ SortDate æ¬„ä½
        final_df.drop(columns=["SortDate"], inplace=True)

        # âœ… ä¿®æ­£éœ€æ±‚ 5: æ¬„ä½ä¸­æ–‡åŒ–
        final_df.rename(columns={
            "Market": "å¸‚å ´",
            "Code": "ä»£è™Ÿ",
            "Name": "åç¨±",
            "Period": "è™•ç½®æœŸé–“",
            "Reason": "è™•ç½®åŸå› "
        }, inplace=True)
        
        return final_df
    else:
        print("âŒ ç„¡è™•ç½®è‚¡è³‡æ–™")
        return pd.DataFrame()

# ============================
# Main
# ============================
def main():
    sh, _ = connect_google_sheets()
    if not sh: return

    update_market_monitoring_log(sh)

    cal_dates = get_official_trading_calendar(240)

    # âœ… [ä¿®æ­£] main() ä¿®æ­£ T-2 å›æœ” Bug
    target_trade_date_obj = cal_dates[-1]
    is_today_trade = (target_trade_date_obj == TARGET_DATE.date())

    # åªæœ‰ã€Œæ—¥æ›†å·²åŒ…å«ä»Šå¤©ã€ä¸”ã€Œç¾åœ¨ < 17:30ã€æ‰é€€å› T-1
    if is_today_trade and (not IS_AFTER_SAFE) and len(cal_dates) >= 2:
        print(f"â³ ç¾åœ¨æ™‚é–“ {TARGET_DATE.strftime('%H:%M')} æ—©æ–¼ {SAFE_CRAWL_TIME}ï¼Œä¸”æ—¥æ›†åŒ…å«ä»Šæ—¥ï¼Œåˆ‡æ›ç‚º T-1 æ¨¡å¼ã€‚")
        target_trade_date_obj = cal_dates[-2]

    target_date_str = target_trade_date_obj.strftime("%Y-%m-%d")
    print(f"ğŸ“… æœ€çµ‚é–å®šé‹ç®—æ—¥æœŸ: {target_date_str}")

    ws_log = get_or_create_ws(sh, "æ¯æ—¥ç´€éŒ„", headers=['æ—¥æœŸ','å¸‚å ´','ä»£è™Ÿ','åç¨±','è§¸çŠ¯æ¢æ¬¾'])

    # âœ… åŸ·è¡Œå›è£œ (åŒ…å«æª¢æŸ¥ç‹€æ…‹è¡¨ç¼ºå¤±)
    backfill_daily_logs(sh, ws_log, cal_dates, target_trade_date_obj)

    print("ğŸ“Š è®€å–æ­·å² Log...")
    log_data = ws_log.get_all_records()
    df_log = pd.DataFrame(log_data)
    if not df_log.empty:
        df_log['ä»£è™Ÿ'] = df_log['ä»£è™Ÿ'].astype(str).str.strip().str.replace("'", "")
        # âœ… [ä¿®æ­£] å¼·åˆ¶æ—¥æœŸæ¨™æº–åŒ– (YYYY-MM-DD)ï¼Œè§£æ±º Google Sheets æ ¼å¼æ··äº‚å•é¡Œ
        df_log['æ—¥æœŸ'] = pd.to_datetime(df_log['æ—¥æœŸ'], errors='coerce').dt.strftime("%Y-%m-%d")
        df_log = df_log[df_log['æ—¥æœŸ'].notna()]

    clause_map = {}
    for _, r in df_log.iterrows():
        key = (str(r['ä»£è™Ÿ']), str(r['æ—¥æœŸ']))
        clause_map[key] = merge_clause_text(clause_map.get(key,""), str(r['è§¸çŠ¯æ¢æ¬¾']))

    # âœ… [ä¿®æ­£] jail_map æ”¹ç‚ºå¾ Sheet å¿«å–è®€å–ï¼Œä¸å†çˆ¬èŸ²
    # jail_map = get_jail_map(target_trade_date_obj - timedelta(days=90), target_trade_date_obj) # (ç§»é™¤èˆŠé‚è¼¯)
    jail_map = get_jail_map_from_sheet(sh)
    
    exclude_map = build_exclude_map(cal_dates, jail_map)

    start_dt_str = cal_dates[-90].strftime("%Y-%m-%d")
    df_recent = df_log[df_log['æ—¥æœŸ'] >= start_dt_str]
    target_stocks = df_recent['ä»£è™Ÿ'].unique()

    precise_db = load_precise_db_from_sheet(sh)
    rows_stats = []

    print(f"ğŸ” æƒæ {len(target_stocks)} æª”è‚¡ç¥¨...")
    for idx, code in enumerate(target_stocks):
        code = str(code).strip()
        name = df_log[df_log['ä»£è™Ÿ']==code]['åç¨±'].iloc[-1] if not df_log[df_log['ä»£è™Ÿ']==code].empty else "æœªçŸ¥"

        db_info = precise_db.get(code, {})
        m_type = str(db_info.get('market', 'ä¸Šå¸‚')).upper()
        suffix = '.TWO' if any(k in m_type for k in ['ä¸Šæ«ƒ', 'TWO', 'TPEX', 'OTC']) else '.TW'
        ticker_code = f"{code}{suffix}"

        stock_calendar = get_last_n_non_jail_trade_dates(code, cal_dates, jail_map, exclude_map, 30)

        bits = []; clauses = []
        for d in stock_calendar:
            c = clause_map.get((code, d.strftime("%Y-%m-%d")), "")
            if is_excluded(code, d, exclude_map):
                bits.append(0); clauses.append(c); continue
            if c: bits.append(1); clauses.append(c)
            else: bits.append(0); clauses.append("")

        # âœ… [ä¿®æ­£] å¼·åˆ¶ enable_safe_filter=False (å‰›å‡ºé—œä¸è¢«æ¿¾æ‰)
        est_days, reason = simulate_days_to_jail_strict(
            bits, clauses, 
            stock_id=code, 
            target_date=target_trade_date_obj, 
            jail_map=jail_map,
            enable_safe_filter=False
        )

        latest_ids = parse_clause_ids_strict(clauses[-1] if clauses else "")
        is_special_risk = is_special_risk_day(latest_ids)
        is_clause_13 = False
        for c in clauses:
            if 13 in parse_clause_ids_strict(c):
                is_clause_13 = True
                break

        est_days_int = 99
        est_days_display = "X"
        reason_display = ""

        if reason == "X":
            est_days_int = 99
            est_days_display = "X"
            if is_special_risk:
                reason_display = "ç±Œç¢¼ç•°å¸¸(äººå·¥å¯©æ ¸é¢¨éšª)"
                if is_clause_13: reason_display += " + åˆ‘æœŸå¯èƒ½å»¶é•·"
        elif est_days == 0:
            est_days_int = 0
            est_days_display = "0"
            reason_display = reason
        else:
            est_days_int = int(est_days)
            est_days_display = str(est_days_int)
            reason_display = reason
            if is_special_risk:
                reason_display += " | âš ï¸ç•™æ„äººå·¥è™•ç½®é¢¨éšª"
            if is_clause_13:
                reason_display += " (è‹¥é€²è™•ç½®å°‡é—œ12å¤©)"

        hist = fetch_history_data(ticker_code)
        if hist.empty:
            alt_s = '.TWO' if suffix=='.TW' else '.TW'
            hist = fetch_history_data(f"{code}{alt_s}")
            if not hist.empty: ticker_code = f"{code}{alt_s}"

        fund = fetch_stock_fundamental(code, ticker_code, precise_db)

        # âœ… ç•¶æ²–ç‡æŠ“å–åˆ¤æ–·ï¼šåªæœ‰éäº† 21:00 æ‰æŠ“ï¼Œå¦å‰‡çµ¦ None
        dt_today, dt_avg6 = None, None
        if IS_AFTER_DAYTRADE:
            dt_today, dt_avg6 = get_daytrade_stats_finmind(code, target_date_str)

        risk = calculate_full_risk(code, hist, fund, est_days_int, dt_today, dt_avg6)

        # streak
        valid_bits = [1 if b==1 and is_valid_accumulation_day(parse_clause_ids_strict(c)) else 0 for b,c in zip(bits, clauses)]
        streak = 0
        for v in reversed(valid_bits):
            if v: streak+=1
            else: break

        status_30 = "".join(map(str, valid_bits)).zfill(30)

        def safe(v):
            if v is None: return ""
            try: 
                if np.isnan(v): return ""
            except: pass
            return str(v)

        # âœ… [ä¿®æ­£] å¢åŠ ä¿è­·ï¼Œé¿å… stock_calendar ç©ºå€¼å– [-1] éŒ¯èª¤
        last_date_val = ""
        if stock_calendar:
            last_date_val = stock_calendar[-1].strftime("%Y-%m-%d")

        row = [
            f"'{code}", name, safe(streak), safe(sum(valid_bits)), safe(sum(valid_bits[-10:])),
            last_date_val, # ä½¿ç”¨ä¿è­·å¾Œçš„è®Šæ•¸
            f"'{status_30}", f"'{status_30[-10:]}", est_days_display, safe(reason_display),
            safe(risk['risk_level']), safe(risk['trigger_msg']),
            safe(risk['curr_price']), safe(risk['limit_price']), safe(risk['gap_pct']),
            safe(risk['curr_vol']), safe(risk['limit_vol']), safe(risk['turnover_val']),
            safe(risk['turnover_rate']), safe(risk['pe']), safe(risk['pb']), safe(risk['day_trade_pct'])
        ]
        rows_stats.append(row)
        if (idx+1)%10==0: time.sleep(1)

    if rows_stats:
        print("ğŸ’¾ æ›´æ–°çµ±è¨ˆè¡¨...")
        ws_stats = get_or_create_ws(sh, "è¿‘30æ—¥ç†±é–€çµ±è¨ˆ", headers=STATS_HEADERS)
        ws_stats.clear()
        ws_stats.append_row(STATS_HEADERS, value_input_option='USER_ENTERED')
        ws_stats.append_rows(rows_stats, value_input_option='USER_ENTERED')
        print("âœ… å®Œæˆ")
    
    # ==========================================
    # ğŸ”¥ æ–°å¢ä»»å‹™ï¼šåŸ·è¡Œè¿‘ 90 æ—¥è™•ç½®è‚¡æŠ“å–ä¸¦å¯«å…¥æ–°å·¥ä½œè¡¨
    # ==========================================
    print("\n" + "="*50)
    print("ğŸš€ å•Ÿå‹•é¡å¤–ä»»å‹™ï¼šæŠ“å–è¿‘ 90 æ—¥è™•ç½®è‚¡æ¸…å–® (Playwright)...")
    print("="*50)
    
    try:
        # ä½¿ç”¨ asyncio.run åŸ·è¡ŒéåŒæ­¥çš„ Playwright çˆ¬èŸ²æµç¨‹
        df_jail_90 = asyncio.run(run_jail_crawler_pipeline())
        
        if not df_jail_90.empty:
            sheet_title = "è™•ç½®è‚¡90æ—¥æ˜ç´°"
            print(f"ğŸ’¾ æ­£åœ¨å¯«å…¥ Google Sheet: {sheet_title}...")
            
            # å®šç¾©éœ€è¦çš„æ¬„ä½é †åº (ä¸­æ–‡æ¬„ä½)
            export_cols = ["å¸‚å ´", "ä»£è™Ÿ", "åç¨±", "è™•ç½®æœŸé–“", "è™•ç½®åŸå› "]
            
            # æº–å‚™å¯«å…¥è³‡æ–™
            final_rows = [export_cols] + df_jail_90[export_cols].values.tolist()
            
            # å¯«å…¥å·¥ä½œè¡¨
            ws_jail = get_or_create_ws(sh, sheet_title, headers=export_cols)
            ws_jail.clear()
            ws_jail.append_rows(final_rows, value_input_option='USER_ENTERED')
            print(f"âœ… {sheet_title} æ›´æ–°å®Œæˆï¼")
        else:
            print("âš ï¸ æŸ¥ç„¡è™•ç½®è‚¡è³‡æ–™ï¼Œè·³éå¯«å…¥ã€‚")
            
    except Exception as e:
        print(f"âŒ è™•ç½®è‚¡çˆ¬èŸ²ä»»å‹™å¤±æ•—: {e}")

if __name__ == "__main__":
    main()
