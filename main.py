# -*- coding: utf-8 -*-
"""
V116.24 å°è‚¡æ³¨æ„è‚¡ç³»çµ± (ä¿®æ­£å³å°‡å‡ºé—œé‚è¼¯)
ä¿®æ­£é‡é»ï¼š
1. [ä¿®æ­£] ã€Œå³å°‡å‡ºé—œç›£æ§ã€é‚è¼¯å„ªåŒ–ï¼šé‡å°åŒä¸€æª”è‚¡ç¥¨æœ‰å¤šç­†è™•ç½®ç´€éŒ„ï¼ˆå¦‚å»¶é•·è™•ç½®ã€äºŒæ¬¡è™•ç½®ï¼‰çš„æƒ…æ³ï¼Œ
   æ”¹ç‚ºå–ã€Œæœ€æ™šçµæŸæ—¥æœŸã€ä¾†è¨ˆç®—å‰©é¤˜å¤©æ•¸ã€‚
   - é¿å…ã€ŒèˆŠæ¡ˆå¿«çµæŸï¼Œä½†æ–°æ¡ˆå‰›é–‹å§‹ã€çš„è‚¡ç¥¨èª¤åˆ¤ç‚ºå³å°‡å‡ºé—œã€‚
2. [ä¿ç•™] ä¸Šæ«ƒ Requests API çˆ¬èŸ²ã€ä¸Šå¸‚ Selenium çˆ¬èŸ²èˆ‡é¢¨éšªè¨ˆç®—é‚è¼¯ã€‚
"""

import os
import twstock
import yfinance as yf
import pandas as pd
import numpy as np
import requests
import re
import time
import random
import gspread
import logging
import nest_asyncio
from google.oauth2.service_account import Credentials
from datetime import datetime, timedelta, time as dt_time, date
from dateutil.relativedelta import relativedelta
from zoneinfo import ZoneInfo
from workalendar.asia import Taiwan

# âœ… Selenium æ¨¡çµ„ (ä¿ç•™çµ¦ TWSE ä½¿ç”¨)
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

nest_asyncio.apply()

# ==========================================
# 1. è¨­å®šéœéŸ³æ¨¡å¼èˆ‡å¸¸æ•¸
# ==========================================
logger = logging.getLogger('yfinance')
logger.setLevel(logging.CRITICAL)
logger.disabled = True

UNIT_LOT = 1000

# å®šç¾©çµ±è¨ˆè¡¨é ­
STATS_HEADERS = [
    'ä»£è™Ÿ', 'åç¨±', 'é€£çºŒå¤©æ•¸', 'è¿‘30æ—¥æ³¨æ„æ¬¡æ•¸', 'è¿‘10æ—¥æ³¨æ„æ¬¡æ•¸', 'æœ€è¿‘ä¸€æ¬¡æ—¥æœŸ',
    '30æ—¥ç‹€æ…‹ç¢¼', '10æ—¥ç‹€æ…‹ç¢¼', 'æœ€å¿«è™•ç½®å¤©æ•¸', 'è™•ç½®è§¸ç™¼åŸå› ', 'é¢¨éšªç­‰ç´š', 'è§¸ç™¼æ¢ä»¶',
    'ç›®å‰åƒ¹', 'è­¦æˆ’åƒ¹', 'å·®å¹…(%)', 'ç›®å‰é‡', 'è­¦æˆ’é‡', 'æˆäº¤å€¼(å„„)',
    'é€±è½‰ç‡(%)', 'PE', 'PB', 'ç•¶æ²–ä½”æ¯”(%)'
]

# ==========================================
# ğŸ“† è¨­å®šå€
# ==========================================
SHEET_NAME = "å°è‚¡æ³¨æ„è‚¡è³‡æ–™åº«_V33"
PARAM_SHEET_NAME = "å€‹è‚¡åƒæ•¸"
TW_TZ = ZoneInfo("Asia/Taipei")
TARGET_DATE = datetime.now(TW_TZ)

# æ™‚é–“é–€æª»
SAFE_CRAWL_TIME = dt_time(17, 30)        
DAYTRADE_PUBLISH_TIME = dt_time(21, 0)   
SAFE_MARKET_OPEN_CHECK = dt_time(16, 30) 

IS_NIGHT_RUN = TARGET_DATE.hour >= 20
IS_AFTER_SAFE = TARGET_DATE.time() >= SAFE_CRAWL_TIME
IS_AFTER_DAYTRADE = TARGET_DATE.time() >= DAYTRADE_PUBLISH_TIME

# å›è£œåƒæ•¸
MAX_BACKFILL_TRADING_DAYS = 40   
VERIFY_RECENT_DAYS = 2              

# ==========================================
# ğŸ”‘ FinMind é‡‘é‘°è¨­å®š
# ==========================================
FINMIND_API_URL = "https://api.finmindtrade.com/api/v4/data"

token1 = os.getenv('FinMind_1')
token2 = os.getenv('FinMind_2')
FINMIND_TOKENS = [t for t in [token1, token2] if t]

CURRENT_TOKEN_INDEX = 0
_FINMIND_CACHE = {}

print(f"ğŸš€ å•Ÿå‹• V116.24 å°è‚¡æ³¨æ„è‚¡ç³»çµ± (Jail Release Fix)")
print(f"ğŸ•’ ç³»çµ±æ™‚é–“ (Taiwan): {TARGET_DATE.strftime('%Y-%m-%d %H:%M:%S')}")

try: twstock.__update_codes()
except: pass

# ============================
# ğŸ› ï¸ å·¥å…·å‡½å¼
# ============================
CN_NUM = {"ä¸€":"1","äºŒ":"2","ä¸‰":"3","å››":"4","äº”":"5","å…­":"6","ä¸ƒ":"7","å…«":"8","ä¹":"9","å":"10"}

KEYWORD_MAP = {
    "èµ·è¿„å…©å€‹ç‡Ÿæ¥­æ—¥": 11, "ç•¶æ—¥æ²–éŠ·": 13, "å€Ÿåˆ¸è³£å‡º": 12, "ç´¯ç©é€±è½‰ç‡": 10, "é€±è½‰ç‡": 4,
    "æˆäº¤é‡": 9, "æœ¬ç›Šæ¯”": 6, "è‚¡åƒ¹æ·¨å€¼æ¯”": 6, "æº¢æŠ˜åƒ¹": 8, "æ”¶ç›¤åƒ¹æ¼²è·Œç™¾åˆ†æ¯”": 1,
    "æœ€å¾Œæˆäº¤åƒ¹æ¼²è·Œ": 1, "æœ€è¿‘å…­å€‹ç‡Ÿæ¥­æ—¥ç´¯ç©": 1
}

def normalize_clause_text(s: str) -> str:
    if not s: return ""
    s = str(s)
    s = s.replace("ç¬¬ã„§æ¬¾", "ç¬¬ä¸€æ¬¾")
    for cn, dg in CN_NUM.items():
        s = s.replace(f"ç¬¬{cn}æ¬¾", f"ç¬¬{dg}æ¬¾")
    s = s.translate(str.maketrans("ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼", "1234567890"))
    return s

def parse_clause_ids_strict(clause_text):
    if not isinstance(clause_text, str): return set()
    clause_text = normalize_clause_text(clause_text)
    ids = set()
    matches = re.findall(r'ç¬¬\s*(\d+)\s*æ¬¾', clause_text)
    for m in matches: ids.add(int(m))
    if not ids:
        for keyword, code in KEYWORD_MAP.items():
            if keyword in clause_text: ids.add(code)
    return ids

def merge_clause_text(a, b):
    ids = set()
    ids |= parse_clause_ids_strict(a) if a else set()
    ids |= parse_clause_ids_strict(b) if b else set()
    if ids: return "ã€".join([f"ç¬¬{x}æ¬¾" for x in sorted(ids)])
    a = a or ""; b = b or ""
    return a if len(a) >= len(b) else b

def is_valid_accumulation_day(ids):
    if not ids: return False
    return any(1 <= x <= 8 for x in ids)

def is_special_risk_day(ids):
    if not ids: return False
    return any(9 <= x <= 14 for x in ids)

def get_ticker_suffix(market_type):
    m = str(market_type).upper().strip()
    keywords = ['ä¸Šæ«ƒ', 'TWO', 'TPEX', 'OTC']
    if any(k in m for k in keywords): return '.TWO'
    return '.TW'

def connect_google_sheets():
    try:
        if not os.path.exists("service_key.json"): return None, None
        gc = gspread.service_account(filename="service_key.json")
        try: sh = gc.open(SHEET_NAME)
        except: sh = gc.create(SHEET_NAME)
        return sh, None
    except: return None, None

def get_or_create_ws(sh, title, headers=None, rows=5000, cols=20):
    need_cols = max(cols, len(headers) if headers else 0)
    try:
        ws = sh.worksheet(title)
        try:
            if headers and ws.col_count < need_cols:
                ws.resize(rows=ws.row_count, cols=need_cols)
        except: pass
        return ws
    except:
        print(f"âš ï¸ å·¥ä½œè¡¨ '{title}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨å»ºç«‹...")
        ws = sh.add_worksheet(title=title, rows=str(rows), cols=str(need_cols))
        if headers:
            ws.append_row(headers, value_input_option="USER_ENTERED")
        return ws

def load_log_index(ws_log):
    existing_keys = set()
    date_counts = {}
    try:
        vals = ws_log.get_all_values()
        if not vals or len(vals) <= 1: return existing_keys, date_counts
        for r in vals[1:]:
            if len(r) >= 3 and str(r[0]).strip():
                d = str(r[0]).strip()
                code = str(r[2]).strip().replace("'", "")
                if code:
                    k = d + "_" + code
                    existing_keys.add(k)
                    date_counts[d] = date_counts.get(d, 0) + 1
    except: pass
    return existing_keys, date_counts

def load_status_index(ws_status):
    key_to_row = {}
    cnt_map = {}
    try:
        vals = ws_status.get_all_values()
        if not vals or len(vals) <= 1: return key_to_row, cnt_map
        for r_idx, row in enumerate(vals[1:], start=2):
            if len(row) >= 1 and str(row[0]).strip():
                d = str(row[0]).strip()
                key_to_row[d] = r_idx
                c = 0
                if len(row) >= 2:
                    try: c = int(str(row[1]).strip())
                    except: c = 0
                cnt_map[d] = c
    except: pass
    return key_to_row, cnt_map

def upsert_status(ws_status, key_to_row, date_str, count, now_str):
    row_data = [date_str, int(count), now_str]
    if date_str in key_to_row:
        r = key_to_row[date_str]
        try: ws_status.update(values=[row_data], range_name=f"A{r}:C{r}", value_input_option="USER_ENTERED")
        except: pass
    else:
        try: ws_status.append_row(row_data, value_input_option="USER_ENTERED")
        except: pass

def finmind_get(dataset, data_id=None, start_date=None, end_date=None):
    global CURRENT_TOKEN_INDEX
    cache_key = (dataset, data_id, start_date, end_date)
    if cache_key in _FINMIND_CACHE: return _FINMIND_CACHE[cache_key].copy()

    params = {"dataset": dataset}
    if data_id: params["data_id"] = str(data_id)
    if start_date: params["start_date"] = start_date
    if end_date: params["end_date"] = end_date
    if not FINMIND_TOKENS: return pd.DataFrame()

    for _ in range(4):
        headers = {"Authorization": f"Bearer {FINMIND_TOKENS[CURRENT_TOKEN_INDEX]}", "User-Agent": "Mozilla/5.0", "Connection": "close"}
        try:
            r = requests.get(FINMIND_API_URL, params=params, headers=headers, timeout=10)
            if r.status_code == 200:
                j = r.json()
                df = pd.DataFrame(j.get("data", [])) if "data" in j else pd.DataFrame()
                if len(_FINMIND_CACHE) >= 2000: _FINMIND_CACHE.clear()
                _FINMIND_CACHE[cache_key] = df
                return df.copy()
            elif r.status_code != 200:
                time.sleep(2)
                CURRENT_TOKEN_INDEX = (CURRENT_TOKEN_INDEX + 1) % len(FINMIND_TOKENS)
                continue
        except: time.sleep(1)
    return pd.DataFrame()

def update_market_monitoring_log(sh):
    print("ğŸ“Š æª¢æŸ¥ä¸¦æ›´æ–°ã€Œå¤§ç›¤æ•¸æ“šç›£æ§ã€...")
    HEADERS = ['æ—¥æœŸ', 'ä»£è™Ÿ', 'åç¨±', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œå¹…(%)', 'æˆäº¤é‡‘é¡(å„„)']
    ws_market = get_or_create_ws(sh, "å¤§ç›¤æ•¸æ“šç›£æ§", headers=HEADERS, cols=10)

    def norm_date(s):
        s = str(s).strip()
        if not s: return ""
        try: return pd.to_datetime(s, errors='coerce').strftime("%Y-%m-%d")
        except: return s

    key_to_row = {}
    try:
        all_vals = ws_market.get_all_values()
        for r_idx, row in enumerate(all_vals[1:], start=2):
            if len(row) >= 2:
                key_to_row[f"{norm_date(row[0])}_{str(row[1]).strip()}"] = r_idx
    except: pass

    existing_keys = set(key_to_row.keys())

    try:
        targets = [
            {'fin_id': 'TAIEX', 'code': '^TWII', 'name': 'åŠ æ¬ŠæŒ‡æ•¸'},
            {'fin_id': 'TPEx',  'code': '^TWOII', 'name': 'æ«ƒè²·æŒ‡æ•¸'}
        ]
        start_date_str = (TARGET_DATE - timedelta(days=45)).strftime("%Y-%m-%d")
        dfs = {}
        for t in targets:
            df = finmind_get("TaiwanStockPrice", data_id=t['fin_id'], start_date=start_date_str)
            if not df.empty:
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
                df.index = df.index.tz_localize(None)
                if 'close' in df.columns:
                    df['Close'] = df['close'].astype(float)
                    df['Pct'] = df['Close'].pct_change() * 100
                if 'Turnover' in df.columns: df['Volume'] = df['Turnover'].astype(float)
                elif 'Trading_money' in df.columns: df['Volume'] = df['Trading_money'].astype(float)
                else: df['Volume'] = 0.0
                dfs[t['code']] = df

        new_rows = []
        today_str = TARGET_DATE.strftime("%Y-%m-%d")
        all_dates = set()
        for df in dfs.values(): all_dates.update(df.index.strftime("%Y-%m-%d").tolist())

        for d in sorted(all_dates):
            for t in targets:
                code = t['code']; name = t['name']
                df = dfs.get(code)
                if df is None or d not in df.index.strftime("%Y-%m-%d"): continue
                try: row = df.loc[d]
                except: row = df[df.index.strftime("%Y-%m-%d") == d].iloc[0]

                if pd.isna(row.get('Close')): continue
                close = round(float(row['Close']), 2)
                pct = round(float(row.get('Pct', 0) or 0), 2)
                vol = round(float(row.get('Volume', 0) or 0) / 100000000, 2)

                row_data = [d, code, name, close, pct, vol]
                comp_key = f"{d}_{code}"

                if d == today_str and TARGET_DATE.time() < SAFE_MARKET_OPEN_CHECK: continue
                if d == today_str and comp_key in key_to_row and TARGET_DATE.time() >= SAFE_MARKET_OPEN_CHECK:
                    try:
                        r_num = key_to_row[comp_key]
                        ws_market.update(values=[row_data], range_name=f'A{r_num}:F{r_num}', value_input_option="USER_ENTERED")
                    except: pass
                    continue
                if comp_key in existing_keys: continue
                if close > 0: new_rows.append(row_data)

        if new_rows: ws_market.append_rows(new_rows, value_input_option="USER_ENTERED")
    except Exception as e: print(f" âŒ å¤§ç›¤æ›´æ–°å¤±æ•—: {e}")

# ============================
# ğŸ”¥ è™•ç½®è³‡æ–™ç›¸é—œå‡½å¼ (Jail)
# ============================
def parse_roc_date(roc_date_str):
    try:
        roc_date_str = str(roc_date_str).strip()
        parts = re.split(r"[/-]", roc_date_str)
        if len(parts) == 3:
            y = int(parts[0]) + 1911
            m = int(parts[1])
            d = int(parts[2])
            return date(y, m, d)
    except:
        return None
    return None

def parse_jail_period(period_str):
    if not period_str:
        return None, None

    s = str(period_str).strip()
    dates = []
    if "ï½" in s:
        dates = s.split("ï½")
    elif "~" in s:
        dates = s.split("~")
    elif "-" in s and "/" in s and s.count("-") == 1:
        dates = s.split("-")

    if len(dates) >= 2:
        sd = parse_roc_date(dates[0].strip())
        ed = parse_roc_date(dates[1].strip())
        if sd and ed:
            return sd, ed
    return None, None

def get_jail_map_from_sheet(sh):
    print("ğŸ“‚ å¾ Google Sheet è®€å–è™•ç½®åå–®å¿«å– (è™•ç½®è‚¡90æ—¥æ˜ç´°)...")
    jail_map = {}
    try:
        ws = sh.worksheet("è™•ç½®è‚¡90æ—¥æ˜ç´°")
        rows = ws.get_all_records()
        for r in rows:
            code = str(r.get('ä»£è™Ÿ', '')).strip()
            if not code: 
                code = str(r.get('Code', '')).strip()
            
            if not code: continue
            
            period = str(r.get('è™•ç½®æœŸé–“', '')).strip()
            if not period:
                period = str(r.get('Period', '')).strip()

            sd, ed = parse_jail_period(period)
            if sd and ed:
                jail_map.setdefault(code, []).append((sd, ed))
        print(f"âœ… å¿«å–è®€å–å®Œæˆï¼Œå…± {len(jail_map)} æª”è™•ç½®è‚¡è³‡æ–™ã€‚")
    except Exception as e:
        print(f"âš ï¸ è®€å–è™•ç½®å¿«å–å¤±æ•— (å¯èƒ½æ˜¯åˆæ¬¡åŸ·è¡Œæˆ–å·¥ä½œè¡¨ä¸å­˜åœ¨): {e}")
    return jail_map

def is_in_jail(stock_id, target_date, jail_map):
    if not jail_map or stock_id not in jail_map:
        return False
    for s, e in jail_map[stock_id]:
        if s <= target_date <= e:
            return True
    return False

def prev_trade_date(d, cal_dates):
    try:
        idx = cal_dates.index(d)
        return cal_dates[idx - 1] if idx > 0 else None
    except:
        for i in range(len(cal_dates) - 1, -1, -1):
            if cal_dates[i] < d:
                return cal_dates[i]
        return None

def build_exclude_map(cal_dates, jail_map):
    exclude_map = {}
    if not jail_map:
        return exclude_map

    for code, periods in jail_map.items():
        s = set()
        for start, end in periods:
            for d in cal_dates:
                if start <= d <= end:
                    s.add(d)
        exclude_map[code] = s
    return exclude_map

def is_excluded(code, d, exclude_map):
    return bool(exclude_map) and (code in exclude_map) and (d in exclude_map[code])

def get_last_n_non_jail_trade_dates(stock_id, cal_dates, jail_map, exclude_map=None, n=30, target_date=None):
    last_jail_end = date(1900, 1, 1)

    if jail_map and stock_id in jail_map:
        past_jail_ends = [e for (s, e) in jail_map[stock_id] if e < target_date]
        if past_jail_ends:
            last_jail_end = max(past_jail_ends)

    picked = []
    for d in reversed(cal_dates):
        if d <= last_jail_end:
            break
        if exclude_map and is_excluded(stock_id, d, exclude_map):
            continue
        if jail_map and is_in_jail(stock_id, d, jail_map):
            continue
        picked.append(d)
        if len(picked) >= n:
            break

    return list(reversed(picked))

def get_last_jail_end(stock_id, target_date, jail_map):
    last_end = None
    if not jail_map or stock_id not in jail_map: return None
    for s, e in jail_map[stock_id]:
        if e < target_date:
            last_end = e if (last_end is None or e > last_end) else last_end
    return last_end

# ============================
# ğŸ”¥ æ¯æ—¥å…¬å‘Šçˆ¬èŸ²å€ (TWSE / TPEx)
# ============================
def fetch_twse_attention_rows(date_obj, date_str):
    date_str_nodash = date_obj.strftime("%Y%m%d")
    rows = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(
            "https://www.twse.com.tw/rwd/zh/announcement/notice",
            params={"startDate": date_str_nodash, "endDate": date_str_nodash, "response": "json"},
            headers=headers,
            timeout=10,
        )
        if r.status_code != 200:
            return None 

        d = r.json()
        for i in d.get("data", []) or []:
            code = str(i[1]).strip()
            name = str(i[2]).strip()
            if len(code) == 4 and code.isdigit():
                raw = " ".join([str(x) for x in i])
                ids = parse_clause_ids_strict(raw)
                c_str = "ã€".join([f"ç¬¬{k}æ¬¾" for k in sorted(ids)]) or raw
                rows.append({"æ—¥æœŸ": date_str, "å¸‚å ´": "TWSE", "ä»£è™Ÿ": code, "åç¨±": name, "è§¸çŠ¯æ¢æ¬¾": c_str})
    except:
        return None 
    return rows

def fetch_tpex_attention_rows(date_obj, date_str):
    roc_date = f"{date_obj.year - 1911}/{date_obj.month:02d}/{date_obj.day:02d}"
    url = "https://www.tpex.org.tw/www/zh-tw/bulletin/attention"

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://www.tpex.org.tw/",
        "Origin": "https://www.tpex.org.tw",
        "Accept": "application/json, text/plain, */*",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "X-Requested-With": "XMLHttpRequest",
    }
    payload = {"date": roc_date, "response": "json"}

    s = requests.Session()

    try:
        s.get("https://www.tpex.org.tw/", headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
    except:
        pass

    for attempt in range(1, 4):
        try:
            r = s.post(url, data=payload, headers=headers, timeout=12)
            if r.status_code != 200:
                time.sleep(0.8)
                continue

            res = r.json()

            target = []
            if "tables" in res:
                for t in res["tables"]:
                    target.extend(t.get("data", []) or [])
            else:
                target = res.get("data", []) or []

            rows = []
            for i in target:
                if len(i) <= 5:
                    continue
                row_date = str(i[5]).strip()
                if row_date not in (roc_date, date_str):
                    continue

                code = str(i[1]).strip()
                name = str(i[2]).strip()
                if not (code.isdigit() and len(code) == 4):
                    continue

                raw = " ".join([str(x) for x in i])
                ids = parse_clause_ids_strict(raw)
                c_str = "ã€".join([f"ç¬¬{k}æ¬¾" for k in sorted(ids)]) if ids else raw

                rows.append({"æ—¥æœŸ": date_str, "å¸‚å ´": "TPEx", "ä»£è™Ÿ": code, "åç¨±": name, "è§¸çŠ¯æ¢æ¬¾": c_str})

            return rows
        except:
            time.sleep(0.8)

    return None

def get_daily_data(date_obj):
    date_str = date_obj.strftime("%Y-%m-%d")
    print(f"ğŸ“¡ çˆ¬å–å…¬å‘Š {date_str}...")

    twse_rows = fetch_twse_attention_rows(date_obj, date_str)
    tpex_rows = fetch_tpex_attention_rows(date_obj, date_str)

    if twse_rows is None or tpex_rows is None:
        print("âŒ æŠ“å–å¤±æ•—ï¼ˆå›å‚³ Noneï¼‰ï¼Œæœ¬è¼ªä¸å¯«å…¥ç‹€æ…‹ï¼Œç•™å¾…ä¸‹æ¬¡å›è£œ")
        return None

    rows = []
    rows.extend(twse_rows)
    rows.extend(tpex_rows)

    if rows:
        print(f"âœ… æŠ“åˆ° {len(rows)} æª”")
    else:
        print("âš ï¸ ç„¡è³‡æ–™")
    return rows

def backfill_daily_logs(sh, ws_log, cal_dates, target_trade_date_obj):
    now_str = TARGET_DATE.strftime("%Y-%m-%d %H:%M:%S")
    existing_keys, date_counts = load_log_index(ws_log)
    ws_status = get_or_create_ws(sh, "çˆ¬å–ç‹€æ…‹", headers=["æ—¥æœŸ", "æŠ“åˆ°æª”æ•¸", "æœ€å¾Œæ›´æ–°æ™‚é–“"], cols=5)
    key_to_row, status_cnt = load_status_index(ws_status)
    status_is_new = (len(status_cnt) == 0)

    # if not status_is_new: ...

    key_to_row, status_cnt = load_status_index(ws_status)
    window_dates = cal_dates[-MAX_BACKFILL_TRADING_DAYS:] if len(cal_dates) > MAX_BACKFILL_TRADING_DAYS else cal_dates[:]
    recent_dates = cal_dates[-VERIFY_RECENT_DAYS:] if len(cal_dates) >= VERIFY_RECENT_DAYS else cal_dates[:]
    dates_to_check = sorted(set(window_dates + recent_dates))

    rows_to_append = []
    status_updates = []

    print(f"ğŸ§© å›è£œæª¢æŸ¥ï¼šå…± {len(dates_to_check)} å€‹äº¤æ˜“æ—¥ï¼ˆå«æœ€è¿‘ {VERIFY_RECENT_DAYS} æ—¥å¼·åˆ¶é©—è­‰ï¼‰")

    for d in dates_to_check:
        d_str = d.strftime("%Y-%m-%d")

        if d == TARGET_DATE.date() and TARGET_DATE.time() < SAFE_CRAWL_TIME: continue

        log_cnt = int(date_counts.get(d_str, 0))
        st_cnt = status_cnt.get(d_str, None)
        need_fetch = False

        if d in recent_dates: need_fetch = True
        if (st_cnt is not None) and (log_cnt < int(st_cnt)): need_fetch = True
        if (st_cnt is None) and (log_cnt == 0): need_fetch = True
        if (st_cnt is None) and (d in window_dates): need_fetch = True

        if not need_fetch: continue

        data = get_daily_data(d)

        if data is None:
            print(f"âš ï¸ {d_str} æŠ“å–å¤±æ•—(None)ï¼Œè·³éä¸æ›´æ–°ç‹€æ…‹")
            continue

        official_cnt = len(data)

        for s in data:
            k = f"{s['æ—¥æœŸ']}_{s['ä»£è™Ÿ']}"
            if k not in existing_keys:
                rows_to_append.append([s['æ—¥æœŸ'], s['å¸‚å ´'], f"'{s['ä»£è™Ÿ']}", s['åç¨±'], s['è§¸çŠ¯æ¢æ¬¾']])
                existing_keys.add(k)
                date_counts[s['æ—¥æœŸ']] = date_counts.get(s['æ—¥æœŸ'], 0) + 1

        status_updates.append((d_str, official_cnt, st_cnt))

    if rows_to_append:
        print(f"ğŸ’¾ å›è£œå¯«å…¥ã€Œæ¯æ—¥ç´€éŒ„ã€ï¼š{len(rows_to_append)} ç­†")
        ws_log.append_rows(rows_to_append, value_input_option="USER_ENTERED")
    else:
        print("âœ… æ¯æ—¥ç´€éŒ„ç„¡éœ€å›è£œå¯«å…¥")

    key_to_row, status_cnt = load_status_index(ws_status)
    for d_str, official_cnt, old_st_cnt in status_updates:
        write_cnt = official_cnt
        if official_cnt == 0:
            if old_st_cnt is not None and int(old_st_cnt) > 0: write_cnt = int(old_st_cnt)
            elif int(date_counts.get(d_str, 0)) > 0: write_cnt = int(date_counts[d_str])
        upsert_status(ws_status, key_to_row, d_str, write_cnt, now_str)

def is_market_open_by_finmind(date_str):
    df = finmind_get("TaiwanStockPrice", data_id="2330", start_date=date_str, end_date=date_str)
    return not df.empty

def get_official_trading_calendar(days=60):
    end = TARGET_DATE.strftime("%Y-%m-%d")
    start = (TARGET_DATE - timedelta(days=days*2)).strftime("%Y-%m-%d")
    print("ğŸ“… ä¸‹è¼‰æ—¥æ›†...")
    df = finmind_get("TaiwanStockTradingDate", start_date=start, end_date=end)
    dates = []
    
    if not df.empty:
        df['date'] = pd.to_datetime(df['date']).dt.date
        dates = sorted(df['date'].tolist())
    else:
        cal = Taiwan()
        curr = TARGET_DATE.date()
        while len(dates) < days:
            if cal.is_working_day(curr):
                dates.append(curr)
            curr -= timedelta(days=1)
        dates = sorted(dates)

    today_date = TARGET_DATE.date()
    is_late_enough = TARGET_DATE.time() > SAFE_MARKET_OPEN_CHECK
    
    cal = Taiwan()
    is_today_work = cal.is_working_day(today_date)

    if dates and today_date > dates[-1] and is_today_work:
        if is_late_enough:
            print(f"âš ï¸ æ—¥æ›†ç¼ºæ¼ä»Šæ—¥ ({today_date})ï¼Œé©—è­‰é–‹å¸‚ä¸­...")
            if is_market_open_by_finmind(today_date.strftime("%Y-%m-%d")):
                print(f"âœ… é©—è­‰æˆåŠŸ (2330æœ‰åƒ¹)ï¼Œè£œå…¥ä»Šæ—¥ã€‚")
                dates.append(today_date)
            else:
                print(f"â›” é©—è­‰å¤±æ•— (2330ç„¡åƒ¹)ï¼Œåˆ¤æ–·ç‚ºä¼‘å¸‚æˆ–è³‡æ–™æœªæ›´æ–°ï¼Œä¸è£œå…¥ã€‚")
        else:
            print(f"â³ æ™‚é–“å°šæ—©ï¼Œæš«ä¸å¼·åˆ¶è£œå…¥ä»Šæ—¥æ—¥æ›†ã€‚")

    return dates[-days:]

def get_daytrade_stats_finmind(stock_id, target_date_str):
    end = target_date_str
    start = (datetime.strptime(target_date_str, "%Y-%m-%d") - timedelta(days=15)).strftime("%Y-%m-%d")
    df_dt = finmind_get("TaiwanStockDayTrading", stock_id, start_date=start, end_date=end)
    df_p = finmind_get("TaiwanStockPrice", stock_id, start_date=start, end_date=end)

    if df_dt.empty or df_p.empty: return None, None
    try:
        m = pd.merge(df_p[['date', 'Trading_Volume']], df_dt[['date', 'Volume']], on='date', how='inner')
        if m.empty: return None, None
        m = m.sort_values('date')
        last = m.iloc[-1]
        td = (last['Volume']/last['Trading_Volume']*100) if last['Trading_Volume']>0 else 0
        avg = m.tail(6); sum_v = avg['Volume'].sum(); sum_t = avg['Trading_Volume'].sum()
        avg_td = (sum_v/sum_t*100) if sum_t>0 else 0
        return round(td, 2), round(avg_td, 2)
    except: return None, None

def fetch_history_data(ticker_code):
    try:
        df = yf.Ticker(ticker_code).history(period="1y", auto_adjust=False)
        if df.empty: return pd.DataFrame()
        df.index = df.index.tz_localize(None)
        return df
    except: return pd.DataFrame()

def load_precise_db_from_sheet(sh):
    try:
        ws = sh.worksheet(PARAM_SHEET_NAME)
        data = ws.get_all_records()
        db = {}
        for row in data:
            code = str(row.get('ä»£è™Ÿ', '')).strip()
            if not code: continue
            try: shares = int(str(row.get('ç™¼è¡Œè‚¡æ•¸', 1)).replace(',', ''))
            except: shares = 1
            try: offset = float(row.get('é¡è‚¡æ¼²å¹…ä¿®æ­£', 0.0))
            except: offset = 0.0
            try: turn_avg = float(row.get('åŒé¡è‚¡å¹³å‡é€±è½‰', 5.0))
            except: turn_avg = 5.0
            try: purity = float(row.get('æˆäº¤é‡ç´”åº¦', 1.0))
            except: purity = 1.0
            market = str(row.get('å¸‚å ´', 'ä¸Šå¸‚')).strip()
            db[code] = {"market": market, "shares": shares, "sector_offset": offset, "sector_turn_avg": turn_avg, "vol_purity": purity}
        return db
    except: return {}

def fetch_stock_fundamental(stock_id, ticker_code, precise_db):
    market = 'ä¸Šå¸‚'; shares = 0
    if str(stock_id) in precise_db:
        db = precise_db[str(stock_id)]
        market = db['market']; shares = db['shares']
    data = {'shares': shares, 'market_type': market, 'pe': -1, 'pb': -1}
    try:
        t = yf.Ticker(ticker_code)
        if ".TWO" in ticker_code: data['market_type'] = 'ä¸Šæ«ƒ'
        if data['shares'] <= 1:
            s = t.fast_info.get('shares', None)
            if s: data['shares'] = int(s)
        data['pe'] = t.info.get('trailingPE', t.info.get('forwardPE', 0))
        data['pb'] = t.info.get('priceToBook', 0)
        if data['pe']: data['pe'] = round(data['pe'], 2)
        if data['pb']: data['pb'] = round(data['pb'], 2)
    except: pass
    return data

def calc_pct(curr, ref):
    return ((curr - ref) / ref) * 100 if ref != 0 else 0

def calculate_full_risk(stock_id, hist_df, fund_data, est_days, dt_today_pct, dt_avg6_pct):
    res = {'risk_level': 'ä½', 'trigger_msg': '', 'curr_price': 0, 'limit_price': 0, 'gap_pct': 999.0, 'curr_vol': 0, 'limit_vol': 0, 'turnover_val': 0, 'turnover_rate': 0, 'pe': fund_data.get('pe', 0), 'pb': fund_data.get('pb', 0), 'day_trade_pct': dt_today_pct, 'is_triggered': False}
    if hist_df.empty or len(hist_df) < 7:
        if est_days <= 1: res['risk_level'] = 'é«˜'
        elif est_days <= 2: res['risk_level'] = 'ä¸­'
        return res

    curr_close = float(hist_df.iloc[-1]['Close'])
    curr_vol_shares = float(hist_df.iloc[-1]['Volume'])
    curr_vol_lots = int(curr_vol_shares / UNIT_LOT)
    shares = fund_data.get('shares', 1)
    if shares > 1: turnover = (curr_vol_shares / shares) * 100
    else: turnover = -1.0
    turnover_val_money = curr_close * curr_vol_shares

    res['curr_price'] = round(curr_close, 2)
    res['curr_vol'] = curr_vol_lots
    res['turnover_rate'] = round(turnover, 2)
    res['turnover_val'] = round(turnover_val_money / 100000000, 2)

    triggers = []
    if curr_close < 5: return res

    window_7 = hist_df.tail(7)
    ref_6 = float(window_7.iloc[0]['Close'])
    rise_6 = calc_pct(curr_close, ref_6)
    price_diff_6 = abs(curr_close - ref_6)

    cond_1 = rise_6 > 32
    cond_2 = (rise_6 > 25) and (price_diff_6 >= 50)
    if cond_1: triggers.append(f"ã€ç¬¬ä¸€æ¬¾ã€‘6æ—¥æ¼²{rise_6:.1f}%(>32%)")
    elif cond_2: triggers.append(f"ã€ç¬¬ä¸€æ¬¾ã€‘6æ—¥æ¼²{rise_6:.1f}%ä¸”åƒ¹å·®{price_diff_6:.0f}å…ƒ")

    limit_p = ref_6 * 1.32
    if cond_2: limit_p = min(limit_p, ref_6 * 1.25)
    res['limit_price'] = round(limit_p, 2)
    res['gap_pct'] = round(((limit_p - curr_close)/curr_close)*100, 1)

    if len(hist_df)>=31 and calc_pct(curr_close, float(hist_df.iloc[-31]['Close'])) > 100: triggers.append("ã€ç¬¬äºŒæ¬¾ã€‘30æ—¥æ¼²>100%")
    if len(hist_df)>=61 and calc_pct(curr_close, float(hist_df.iloc[-61]['Close'])) > 130: triggers.append("ã€ç¬¬äºŒæ¬¾ã€‘60æ—¥æ¼²>130%")
    if len(hist_df)>=91 and calc_pct(curr_close, float(hist_df.iloc[-91]['Close'])) > 160: triggers.append("ã€ç¬¬äºŒæ¬¾ã€‘90æ—¥æ¼²>160%")

    if len(hist_df) >= 61:
        avg_vol_60 = hist_df['Volume'].iloc[-61:-1].mean()
        if avg_vol_60 > 0:
            vol_ratio = curr_vol_shares / avg_vol_60
            res['limit_vol'] = int(avg_vol_60 * 5 / 1000)
            if turnover >= 0.1 and curr_vol_lots >= 500:
                if rise_6 > 25 and vol_ratio > 5: triggers.append(f"ã€ç¬¬ä¸‰æ¬¾ã€‘æ¼²{rise_6:.0f}%+é‡{vol_ratio:.1f}å€")

    if turnover > 10 and rise_6 > 25: triggers.append(f"ã€ç¬¬å››æ¬¾ã€‘æ¼²{rise_6:.0f}%+è½‰{turnover:.0f}%")

    if len(hist_df) >= 61:
        avg_vol_60 = hist_df['Volume'].iloc[-61:-1].mean()
        avg_vol_6 = hist_df['Volume'].iloc[-6:].mean()
        is_exclude = (turnover < 0.1) or (curr_vol_lots < 500) or (turnover_val_money < 30000000)
        if not is_exclude and avg_vol_60 > 0:
            r1 = avg_vol_6 / avg_vol_60
            r2 = curr_vol_shares / avg_vol_60
            if r1 > 5: triggers.append(f"ã€ç¬¬ä¹æ¬¾ã€‘6æ—¥å‡é‡æ”¾å¤§{r1:.1f}å€")
            if r2 > 5: triggers.append(f"ã€ç¬¬ä¹æ¬¾ã€‘ç•¶æ—¥é‡æ”¾å¤§{r2:.1f}å€")

    if turnover > 0 and turnover_val_money >= 500000000:
        acc_turn = (hist_df['Volume'].iloc[-6:].sum() / shares) * 100
        if acc_turn > 50 and turnover > 10: triggers.append(f"ã€ç¬¬åæ¬¾ã€‘ç´¯è½‰{acc_turn:.0f}%")

    if len(hist_df) >= 6:
        gap = hist_df.iloc[-6:]['High'].max() - hist_df.iloc[-6:]['Low'].min()
        threshold = 100 + (int((curr_close - 500)/500)+1)*25 if curr_close >= 500 else 100
        if gap >= threshold: triggers.append(f"ã€ç¬¬åä¸€æ¬¾ã€‘6æ—¥åƒ¹å·®{gap:.0f}å…ƒ(>é–€æª»{threshold})")

    pending_msg = ""
    if dt_today_pct is None or dt_avg6_pct is None:
        pending_msg = "(ç•¶æ²–ç‡å¾…å…¬å¸ƒ)"
    else:
        dt_vol_est = curr_vol_shares * (dt_today_pct / 100.0)
        dt_vol_lots = dt_vol_est / 1000
        is_exclude = (turnover < 5) or (turnover_val_money < 500000000) or (dt_vol_lots < 5000)
        if not is_exclude:
            if dt_avg6_pct > 60 and dt_today_pct > 60:
                triggers.append(f"ã€ç¬¬åä¸‰æ¬¾ã€‘ç•¶æ²–{dt_today_pct}%(6æ—¥{dt_avg6_pct}%)")

    if triggers:
        res['is_triggered'] = True
        res['risk_level'] = 'é«˜'
        res['trigger_msg'] = "ä¸”".join(triggers) + (f" {pending_msg}" if pending_msg else "")
    else:
        res['trigger_msg'] = pending_msg
        if est_days <= 1: res['risk_level'] = 'é«˜'
        elif est_days <= 2: res['risk_level'] = 'ä¸­'
        elif est_days >= 3: res['risk_level'] = 'ä½'

    return res

def check_jail_trigger_now(status_list, clause_list):
    status_list = list(status_list); clause_list = list(clause_list)
    if len(status_list) < 30:
        pad = 30 - len(status_list)
        status_list = [0]*pad + status_list
        clause_list = [""]*pad + clause_list

    c1_streak = 0
    for b, c in zip(status_list[-3:], clause_list[-3:]):
        if b == 1 and (1 in parse_clause_ids_strict(c)): 
            c1_streak += 1

    v5 = 0; v10 = 0; v30 = 0
    total = len(status_list)
    for i in range(30):
        idx = total - 1 - i
        if idx < 0: break
        if status_list[idx] == 1:
            ids = parse_clause_ids_strict(clause_list[idx])
            if is_valid_accumulation_day(ids):
                if i < 5: v5 += 1
                if i < 10: v10 += 1
                v30 += 1

    reasons = []
    if c1_streak == 3: reasons.append("å·²è§¸ç™¼(é€£3ç¬¬ä¸€æ¬¾)")
    if v5 == 5: reasons.append("å·²è§¸ç™¼(é€£5)")
    if v10 >= 6: reasons.append(f"å·²è§¸ç™¼(10æ—¥{v10}æ¬¡)")
    if v30 >= 12: reasons.append(f"å·²è§¸ç™¼(30æ—¥{v30}æ¬¡)")
    return (len(reasons) > 0), " | ".join(reasons)

def simulate_days_to_jail_strict(status_list, clause_list, *, stock_id=None, target_date=None, jail_map=None, enable_safe_filter=True):
    if stock_id and target_date and jail_map and is_in_jail(stock_id, target_date, jail_map):
        return 0, "è™•ç½®ä¸­"

    trigger_now, reason_now = check_jail_trigger_now(status_list, clause_list)
    if trigger_now:
        return 0, reason_now.replace("å·²è§¸ç™¼", "å·²é”æ¨™ï¼Œæ¬¡ä¸€ç‡Ÿæ¥­æ—¥è™•ç½®")

    if enable_safe_filter:
        recent_valid_10 = 0
        check_len = min(len(status_list), 10)
        if check_len > 0:
            for b, c in zip(status_list[-check_len:], clause_list[-check_len:]):
                if b == 1 and is_valid_accumulation_day(parse_clause_ids_strict(c)):
                    recent_valid_10 += 1
        if recent_valid_10 == 0: return 99, "X"

    status_list = list(status_list); clause_list = list(clause_list)
    if len(status_list) < 30:
        pad = 30 - len(status_list)
        status_list = [0]*pad + status_list
        clause_list = [""]*pad + clause_list

    days = 0
    while days < 10:
        days += 1
        status_list.append(1); clause_list.append("ç¬¬1æ¬¾")

        c1_streak = 0
        for b, c in zip(status_list[-3:], clause_list[-3:]):
            if b == 1 and (1 in parse_clause_ids_strict(c)): 
                c1_streak += 1

        v5 = 0; v10 = 0; v30 = 0
        total = len(status_list)
        for i in range(30):
            idx = total - 1 - i
            if idx < 0: break
            if status_list[idx] == 1:
                ids = parse_clause_ids_strict(clause_list[idx])
                if is_valid_accumulation_day(ids):
                    if i < 5: v5 += 1
                    if i < 10: v10 += 1
                    v30 += 1

        reasons = []
        if c1_streak == 3: reasons.append(f"å†{days}å¤©è™•ç½®")
        if v5 == 5: reasons.append(f"å†{days}å¤©è™•ç½®(é€£5)")
        if v10 >= 6: reasons.append(f"å†{days}å¤©è™•ç½®(10æ—¥{v10}æ¬¡)")
        if v30 >= 12: reasons.append(f"å†{days}å¤©è™•ç½®(30æ—¥{v30}æ¬¡)")

        if reasons:
            return days, " | ".join(reasons)

    return 99, ""

# ==========================================
# ğŸ”¥ è™•ç½®è‚¡ 90 æ—¥æ˜ç´°çˆ¬èŸ²é‚è¼¯ (Requests + Selenium)
# ==========================================
def get_driver():
    """ å–å¾— Selenium Chrome Driver (Headless) """
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def fetch_tpex_jail_90d_requests(s_date, e_date):
    """
    [æ›¿æ›] ä¸Šæ«ƒ (TPEx) è™•ç½®è‚¡çˆ¬èŸ² - Requests API ç‰ˆ (åƒç…§ä½¿ç”¨è€…æä¾›çš„é‚è¼¯)
    """
    print(f"  [ä¸Šæ«ƒ] å•Ÿå‹• Requests çˆ¬èŸ² (æ–°ç‰ˆå®˜ç¶² API)... {s_date} ~ {e_date}")
    
    # ä¾ç…§ä½¿ç”¨è€…çš„é‚è¼¯ä¿®æ­£ï¼šçµæŸæ—¥æœŸå¼·åˆ¶å¾€å¾Œæ¨ 30 å¤©
    # ç¢ºä¿æŠ“åˆ°ã€Œä»Šæ—¥å…¬å¸ƒã€ä¸‹é€±æ‰é–‹å§‹è™•ç½®ã€çš„è‚¡ç¥¨
    real_end_date = e_date + timedelta(days=30)
    
    sd = f"{s_date.year - 1911}/{s_date.month:02d}/{s_date.day:02d}"
    ed = f"{real_end_date.year - 1911}/{real_end_date.month:02d}/{real_end_date.day:02d}"
    
    url = "https://www.tpex.org.tw/www/zh-tw/bulletin/disposal"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "X-Requested-With": "XMLHttpRequest",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": "https://www.tpex.org.tw/www/zh-tw/bulletin/disposal"
    }
    
    payload = {
        "startDate": sd,
        "endDate": ed,
        "response": "json"
    }
    
    sess = requests.Session()
    clean_data = []
    
    try:
        # 1. Get Cookie
        sess.get(url, headers=headers) 
        
        # 2. Post
        r = sess.post(url, data=payload, headers=headers, timeout=10)
        
        if r.status_code == 200:
            data = r.json()
            if "tables" in data and len(data["tables"]) > 0:
                rows = data["tables"][0].get("data", [])
                print(f"    â””â”€â”€ âš¡ åµæ¸¬åˆ° {len(rows)} ç­†è³‡æ–™...")
                
                for row in rows:
                    # 1: Date, 2: Code, 3: Name(HTML), 5: Period
                    if len(row) < 6: continue
                    c_code = str(row[2]).strip()
                    c_name_raw = str(row[3]).strip()
                    c_name = c_name_raw.split("(")[0] if "(" in c_name_raw else c_name_raw
                    c_period = str(row[5]).strip()
                    
                    if c_code.isdigit() and len(c_code) == 4:
                        clean_data.append({
                            "Code": c_code,
                            "Name": c_name,
                            "Period": c_period,
                            "Market": "ä¸Šæ«ƒ"
                        })
    except Exception as e:
        print(f"    âŒ TPEx Requests å¤±æ•—: {e}")
        
    if clean_data:
        return pd.DataFrame(clean_data)
    return pd.DataFrame()

def fetch_twse_selenium_90d(s_date, e_date):
    """
    [é‡å¯«] ä¸Šå¸‚ (TWSE) è™•ç½®è‚¡çˆ¬èŸ² - Selenium ç‰ˆ
    """
    print(f"  [ä¸Šå¸‚] å•Ÿå‹• Selenium ç€è¦½å™¨... {s_date} ~ {e_date}")
    
    sd_str = s_date.strftime("%Y%m%d")
    ed_str = e_date.strftime("%Y%m%d")
    
    url = "https://www.twse.com.tw/zh/announcement/punish.html"
    driver = get_driver()
    clean_data = []

    try:
        driver.get(url)
        wait = WebDriverWait(driver, 20)
        
        # 1. å¡«å¯«æ—¥æœŸ
        driver.execute_script(f"""
            document.querySelector('input[name="startDate"]').value = "{sd_str}";
            document.querySelector('input[name="endDate"]').value = "{ed_str}";
        """)
        
        # 2. é»æ“ŠæŸ¥è©¢
        search_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "button.search")))
        search_btn.click()
        
        # 3. ç­‰å¾…è¡¨æ ¼å‡ºç¾
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
        time.sleep(3)
        
        # 4. è§£æè¡¨æ ¼
        # ä¸Šå¸‚è¡¨æ ¼çµæ§‹é€šå¸¸æ¯”è¼ƒæ¨™æº–ï¼Œç›´æ¥æŠ“å–
        rows = driver.find_elements(By.CSS_SELECTOR, "table tbody tr")
        print(f"    â””â”€â”€ âš¡ åµæ¸¬åˆ° {len(rows)} ç­†è³‡æ–™ï¼Œé–‹å§‹è§£æ...")
        
        for row in rows:
            try:
                cols = row.find_elements(By.TAG_NAME, "td")
                if len(cols) >= 7:
                    # Index 2: Code
                    # Index 3: Name
                    # Index 6: Period
                    c_code = cols[2].text.strip()
                    c_name = cols[3].text.strip()
                    c_period = cols[6].text.strip()
                    
                    if c_code and c_code.isdigit() and len(c_code) == 4:
                         clean_data.append({
                            "Code": c_code,
                            "Name": c_name,
                            "Period": c_period,
                            "Market": "ä¸Šå¸‚"
                        })
            except: continue
            
    except Exception as e:
        print(f"    âŒ TWSE Selenium æ“ä½œå¤±æ•—: {e}")
    finally:
        driver.quit()

    if clean_data:
        print(f"    âœ… æˆåŠŸè§£æ {len(clean_data)} ç­†è³‡æ–™")
        return pd.DataFrame(clean_data)

    print("    âš ï¸ TWSE ç„¡è³‡æ–™")
    return pd.DataFrame()


def run_jail_crawler_pipeline_sync():
    """ æ•´åˆä¸Šå¸‚æ«ƒè¿‘ 90 æ—¥è™•ç½®è‚¡çˆ¬èŸ²æµç¨‹ (åŒæ­¥ç‰ˆ) """
    end_date = TARGET_DATE.date()
    start_date = end_date - timedelta(days=150)
    print(f"ğŸ¯ å•Ÿå‹•å…¨å¸‚å ´è™•ç½®è‚¡æŠ“å– (TWSE: Selenium / TPEx: Requests): {start_date} ~ {end_date}")

    # ä¾åºåŸ·è¡Œ
    df_tpex = fetch_tpex_jail_90d_requests(start_date, end_date) # æ”¹ç”¨ Requests
    df_twse = fetch_twse_selenium_90d(start_date, end_date) # ç¶­æŒ Selenium
    
    all_dfs = []
    if not df_tpex.empty: all_dfs.append(df_tpex)
    if not df_twse.empty: all_dfs.append(df_twse)

    if all_dfs:
        print("\nğŸ”„ åˆä½µè™•ç½®è‚¡è³‡æ–™ä¸­...")
        final_df = pd.concat(all_dfs, ignore_index=True)
        
        final_df["Code"] = final_df["Code"].astype(str).str.strip()
        final_df["Name"] = final_df["Name"].astype(str).str.strip()
        final_df["Period"] = final_df["Period"].astype(str).str.strip()

        mask_empty_code = (final_df["Code"] == "")
        if mask_empty_code.any():
            print(f"âš ï¸ ç™¼ç¾ {mask_empty_code.sum()} ç­†ä»£è™Ÿç©ºç™½è³‡æ–™ï¼Œå˜—è©¦ä¿®å¾©...")
            extracted = final_df.loc[mask_empty_code, "Name"].str.extract(r'^(\d{4})')
            final_df.loc[mask_empty_code, "Code"] = extracted[0].fillna("")
            final_df.loc[mask_empty_code, "Name"] = final_df.loc[mask_empty_code, "Name"].str.replace(r'^\d{4}\s+', '', regex=True)

        final_df["Code"] = final_df["Code"].astype(str).str.replace(r'\D', '', regex=True)
        final_df = final_df[final_df["Code"].str.match(r'^\d{4}$')]
        
        def parse_sort_date(period_str):
            try:
                start_part = period_str.replace("~", "-").split("-")[0].strip()
                if "/" in start_part:
                    parts = start_part.split("/")
                    if len(parts) == 3:
                        y = int(parts[0]) + 1911
                        m = int(parts[1])
                        d = int(parts[2])
                        return f"{y}{m:02d}{d:02d}"
                return "99999999"
            except:
                return "99999999"

        final_df["SortDate"] = final_df["Period"].apply(parse_sort_date)
        final_df.sort_values(by=["SortDate", "Code"], ascending=[False, True], inplace=True)
        # final_df.drop_duplicates(subset=["Code", "Period"], inplace=True) # ç§»é™¤é€™è¡Œï¼Œä¿ç•™æ‰€æœ‰ç´€éŒ„ä»¥ä¾›å¾ŒçºŒæ¯”å°
        final_df.drop(columns=["SortDate"], inplace=True)

        final_df.rename(columns={
            "Market": "å¸‚å ´",
            "Code": "ä»£è™Ÿ",
            "Name": "åç¨±",
            "Period": "è™•ç½®æœŸé–“"
        }, inplace=True)
        
        return final_df
    else:
        print("âŒ ç„¡è™•ç½®è‚¡è³‡æ–™")
        return pd.DataFrame()

# ============================
# Main
# ============================
def main():
    sh, _ = connect_google_sheets()
    if not sh: return

    print("\n" + "="*50)
    print("ğŸš€ å•Ÿå‹•é¡å¤–ä»»å‹™ï¼šæŠ“å–è¿‘ 90 æ—¥è™•ç½®è‚¡æ¸…å–®...")
    print("="*50)
    
    try:
        # æ”¹ç‚ºå‘¼å«åŒæ­¥ç‰ˆ Pipeline
        df_jail_90 = run_jail_crawler_pipeline_sync()
        
        if not df_jail_90.empty:
            # 1. å¯«å…¥ç¸½è¡¨ (å»é‡å¾Œå¯«å…¥)
            df_jail_unique = df_jail_90.drop_duplicates(subset=["ä»£è™Ÿ", "è™•ç½®æœŸé–“"])
            sheet_title = "è™•ç½®è‚¡90æ—¥æ˜ç´°"
            print(f"ğŸ’¾ æ­£åœ¨å¯«å…¥ Google Sheet: {sheet_title}...")
            export_cols = ["å¸‚å ´", "ä»£è™Ÿ", "åç¨±", "è™•ç½®æœŸé–“"]
            final_rows = [export_cols] + df_jail_unique[export_cols].values.tolist()
            ws_jail = get_or_create_ws(sh, sheet_title, headers=export_cols)
            ws_jail.clear()
            ws_jail.append_rows(final_rows, value_input_option='USER_ENTERED')
            print(f"âœ… {sheet_title} æ›´æ–°å®Œæˆï¼")

            # 2. [æ–°å¢] ç¯©é¸ã€Œå³å°‡å‡ºé—œã€ (å‰©é¤˜ 0~5 å¤©)
            # é‚è¼¯å„ªåŒ–ï¼šè‹¥æœ‰äºŒæ¬¡è™•ç½®ï¼Œéœ€å–ã€Œæœ€æ™šã€çš„çµæŸæ—¥æœŸ
            print("ğŸ” ç¯©é¸å³å°‡å‡ºé—œè‚¡ç¥¨ (5æ—¥å…§)...")
            releasing_rows = []
            today_date = TARGET_DATE.date()
            
            # (A) å»ºç«‹æ¯æª”è‚¡ç¥¨çš„ã€Œæœ€æ™šçµæŸæ—¥æœŸã€å°æ‡‰è¡¨
            stock_latest_end = {} # {code: {'date': max_end_date, 'row': row_data}}

            for idx, row in df_jail_90.iterrows():
                try:
                    p = str(row["è™•ç½®æœŸé–“"]).strip()
                    sd_date, ed_date = parse_jail_period(p)
                    if ed_date:
                        code = str(row["ä»£è™Ÿ"]).strip()
                        # è‹¥å°šæœªè¨˜éŒ„ æˆ– ç•¶å‰ç´€éŒ„æ¯”è¨˜éŒ„ä¸­çš„æ™š -> æ›´æ–°
                        if code not in stock_latest_end or ed_date > stock_latest_end[code]['date']:
                            stock_latest_end[code] = {
                                'date': ed_date,
                                'row': row
                            }
                except: pass

            # (B) æª¢æŸ¥æ¯æª”è‚¡ç¥¨çš„ã€Œæœ€çµ‚çµæŸæ—¥ã€æ˜¯å¦åœ¨ 5 å¤©å…§
            # æ’åºï¼šæŒ‰çµæŸæ—¥æœŸ
            sorted_stocks = sorted(stock_latest_end.items(), key=lambda x: x[1]['date'])

            for code, data in sorted_stocks:
                final_end_date = data['date']
                row_data = data['row']
                
                days_left = (final_end_date - today_date).days
                
                # åªæœ‰ç•¶ã€Œæœ€çµ‚çµæŸæ—¥ã€çœŸçš„å¿«åˆ°äº†ï¼Œæ‰ç®—å³å°‡å‡ºé—œ
                if 0 <= days_left <= 5:
                    r_list = row_data[export_cols].tolist()
                    r_list.append(str(days_left)) # å¢åŠ ã€Œå‰©é¤˜å¤©æ•¸ã€
                    r_list.append(final_end_date.strftime("%Y-%m-%d")) # å¢åŠ ã€Œå‡ºé—œæ—¥ã€
                    releasing_rows.append(r_list)

            sheet_title_release = "å³å°‡å‡ºé—œç›£æ§"
            cols_release = export_cols + ["å‰©é¤˜å¤©æ•¸", "å‡ºé—œæ—¥æœŸ"]
            ws_release = get_or_create_ws(sh, sheet_title_release, headers=cols_release)
            ws_release.clear()
            
            if releasing_rows:
                ws_release.append_row(cols_release, value_input_option='USER_ENTERED')
                ws_release.append_rows(releasing_rows, value_input_option='USER_ENTERED')
                print(f"âœ… å·²å¯«å…¥ {len(releasing_rows)} æª”è‡³ã€Œ{sheet_title_release}ã€")
            else:
                ws_release.append_row(["ç›®å‰ç„¡ 5 æ—¥å…§å³å°‡å‡ºé—œè‚¡ç¥¨"], value_input_option='USER_ENTERED')
                print("âš ï¸ ç›®å‰ç„¡ç¬¦åˆæ¢ä»¶çš„å³å°‡å‡ºé—œè‚¡ã€‚")

        else:
            print("âš ï¸ æŸ¥ç„¡è™•ç½®è‚¡è³‡æ–™ï¼Œè·³éå¯«å…¥ã€‚")
            
    except Exception as e:
        print(f"âŒ è™•ç½®è‚¡çˆ¬èŸ²ä»»å‹™å¤±æ•—: {e}")

    # ============================
    # å¾ŒçºŒåŸ·è¡Œé¢¨éšªè¨ˆç®—èˆ‡ç›£æ§
    # ============================
    update_market_monitoring_log(sh)

    cal_dates = get_official_trading_calendar(240)

    target_trade_date_obj = cal_dates[-1]
    is_today_trade = (target_trade_date_obj == TARGET_DATE.date())

    if is_today_trade and (not IS_AFTER_SAFE) and len(cal_dates) >= 2:
        print(f"â³ ç¾åœ¨æ™‚é–“ {TARGET_DATE.strftime('%H:%M')} æ—©æ–¼ {SAFE_CRAWL_TIME}ï¼Œä¸”æ—¥æ›†åŒ…å«ä»Šæ—¥ï¼Œåˆ‡æ›ç‚º T-1 æ¨¡å¼ã€‚")
        target_trade_date_obj = cal_dates[-2]

    target_date_str = target_trade_date_obj.strftime("%Y-%m-%d")
    print(f"ğŸ“… æœ€çµ‚é–å®šé‹ç®—æ—¥æœŸ: {target_date_str}")

    ws_log = get_or_create_ws(sh, "æ¯æ—¥ç´€éŒ„", headers=['æ—¥æœŸ','å¸‚å ´','ä»£è™Ÿ','åç¨±','è§¸çŠ¯æ¢æ¬¾'])

    backfill_daily_logs(sh, ws_log, cal_dates, target_trade_date_obj)

    print("ğŸ“Š è®€å–æ­·å² Log...")
    log_data = ws_log.get_all_records()
    df_log = pd.DataFrame(log_data)
    if not df_log.empty:
        df_log['ä»£è™Ÿ'] = df_log['ä»£è™Ÿ'].astype(str).str.strip().str.replace("'", "")
        df_log['æ—¥æœŸ'] = pd.to_datetime(df_log['æ—¥æœŸ'], errors='coerce').dt.strftime("%Y-%m-%d")
        df_log = df_log[df_log['æ—¥æœŸ'].notna()]

    clause_map = {}
    for _, r in df_log.iterrows():
        key = (str(r['ä»£è™Ÿ']), str(r['æ—¥æœŸ']))
        clause_map[key] = merge_clause_text(clause_map.get(key,""), str(r['è§¸çŠ¯æ¢æ¬¾']))

    jail_map = get_jail_map_from_sheet(sh)
    
    exclude_map = build_exclude_map(cal_dates, jail_map)

    start_dt_str = cal_dates[-90].strftime("%Y-%m-%d")
    df_recent = df_log[df_log['æ—¥æœŸ'] >= start_dt_str]
    target_stocks = df_recent['ä»£è™Ÿ'].unique()

    precise_db = load_precise_db_from_sheet(sh)
    rows_stats = []

    print(f"ğŸ” æƒæ {len(target_stocks)} æª”è‚¡ç¥¨...")
    for idx, code in enumerate(target_stocks):
        code = str(code).strip()
        name = df_log[df_log['ä»£è™Ÿ']==code]['åç¨±'].iloc[-1] if not df_log[df_log['ä»£è™Ÿ']==code].empty else "æœªçŸ¥"

        db_info = precise_db.get(code, {})
        m_type = str(db_info.get('market', 'ä¸Šå¸‚')).upper()
        suffix = '.TWO' if any(k in m_type for k in ['ä¸Šæ«ƒ', 'TWO', 'TPEX', 'OTC']) else '.TW'
        ticker_code = f"{code}{suffix}"

        stock_calendar = get_last_n_non_jail_trade_dates(
            code, cal_dates, jail_map, exclude_map, 30, target_date=TARGET_DATE.date()
        )

        cutoff = get_last_jail_end(code, TARGET_DATE.date(), jail_map)

        bits = []; clauses = []
        for d in stock_calendar:
            d0 = d 
            
            if cutoff and d0 <= cutoff:
                bits.append(0); clauses.append("")
                continue

            c = clause_map.get((code, d.strftime("%Y-%m-%d")), "")
            if is_excluded(code, d, exclude_map):
                bits.append(0); clauses.append(""); continue
            
            if c: bits.append(1); clauses.append(c)
            else: bits.append(0); clauses.append("")

        est_days, reason = simulate_days_to_jail_strict(
            bits, clauses, 
            stock_id=code, 
            target_date=TARGET_DATE.date(), 
            jail_map=jail_map,
            enable_safe_filter=False
        )

        latest_ids = parse_clause_ids_strict(clauses[-1] if clauses else "")
        is_special_risk = is_special_risk_day(latest_ids)
        is_clause_13 = False
        for c in clauses:
            if 13 in parse_clause_ids_strict(c):
                is_clause_13 = True
                break

        est_days_int = 99
        est_days_display = "X"
        reason_display = ""

        if reason == "X":
            est_days_int = 99
            est_days_display = "X"
            if is_special_risk:
                reason_display = "ç±Œç¢¼ç•°å¸¸(äººå·¥å¯©æ ¸é¢¨éšª)"
                if is_clause_13: reason_display += " + åˆ‘æœŸå¯èƒ½å»¶é•·"
        elif est_days == 0:
            est_days_int = 0
            est_days_display = "0"
            reason_display = reason
        else:
            est_days_int = int(est_days)
            est_days_display = str(est_days_int)
            reason_display = reason
            if is_special_risk:
                reason_display += " | âš ï¸ç•™æ„äººå·¥è™•ç½®é¢¨éšª"
            if is_clause_13:
                reason_display += " (è‹¥é€²è™•ç½®å°‡é—œ12å¤©)"

        hist = fetch_history_data(ticker_code)
        if hist.empty:
            alt_s = '.TWO' if suffix=='.TW' else '.TW'
            hist = fetch_history_data(f"{code}{alt_s}")
            if not hist.empty: ticker_code = f"{code}{alt_s}"

        fund = fetch_stock_fundamental(code, ticker_code, precise_db)

        dt_today, dt_avg6 = None, None
        if IS_AFTER_DAYTRADE:
            dt_today, dt_avg6 = get_daytrade_stats_finmind(code, target_date_str)

        risk = calculate_full_risk(code, hist, fund, est_days_int, dt_today, dt_avg6)

        valid_bits = [1 if b==1 and is_valid_accumulation_day(parse_clause_ids_strict(c)) else 0 for b,c in zip(bits, clauses)]
        streak = 0
        for v in reversed(valid_bits):
            if v: streak+=1
            else: break

        status_30 = "".join(map(str, valid_bits)).zfill(30)

        def safe(v):
            if v is None: return ""
            try: 
                if np.isnan(v): return ""
            except: pass
            return str(v)

        last_date_val = ""
        if stock_calendar:
            last_date_val = stock_calendar[-1].strftime("%Y-%m-%d")

        row = [
            f"'{code}", name, safe(streak), safe(sum(valid_bits)), safe(sum(valid_bits[-10:])),
            last_date_val, 
            f"'{status_30}", f"'{status_30[-10:]}", est_days_display, safe(reason_display),
            safe(risk['risk_level']), safe(risk['trigger_msg']),
            safe(risk['curr_price']), safe(risk['limit_price']), safe(risk['gap_pct']),
            safe(risk['curr_vol']), safe(risk['limit_vol']), safe(risk['turnover_val']),
            safe(risk['turnover_rate']), safe(risk['pe']), safe(risk['pb']), safe(risk['day_trade_pct'])
        ]
        rows_stats.append(row)
        if (idx+1)%10==0: time.sleep(1)

    if rows_stats:
        print("ğŸ’¾ æ›´æ–°çµ±è¨ˆè¡¨...")
        ws_stats = get_or_create_ws(sh, "è¿‘30æ—¥ç†±é–€çµ±è¨ˆ", headers=STATS_HEADERS)
        ws_stats.clear()
        ws_stats.append_row(STATS_HEADERS, value_input_option='USER_ENTERED')
        ws_stats.append_rows(rows_stats, value_input_option='USER_ENTERED')
        print("âœ… å®Œæˆ")

if __name__ == "__main__":
    main()
